{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce81816",
   "metadata": {},
   "source": [
    "# Lesson 4 — The Perceptron Algorithm\n",
    "\n",
    "In this notebook we implement the classic perceptron algorithm for binary\n",
    "classification.  We explore how the perceptron finds a separating hyperplane\n",
    "for linearly separable data and compare it conceptually to logistic regression.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Problem setup & data**: choose a linearly separable subset of a dataset.\n",
    "- **Algorithm derivation**: perceptron update rule.\n",
    "- **Implementation**: train the perceptron and track mistakes.\n",
    "- **Visualization**: plot the decision boundary in two dimensions.\n",
    "- **Exercises & interview summary**: deeper questions and key points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb3ac0",
   "metadata": {},
   "source": [
    "### Imports & Data Preparation\n",
    "\n",
    "We'll use the Iris dataset but restrict to the first two classes (setosa and versicolor)\n",
    "to make the problem binary and linearly separable.  We select the first two features\n",
    "for visualization.  Labels are mapped to \\(\\{-1, +1\\}\\) to simplify the update rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ef99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # datasets only\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X_raw = iris.data[:, :2]  # take first two features for plotting\n",
    "y_raw = iris.target\n",
    "\n",
    "# Keep only classes 0 and 1 (setosa vs versicolor)\n",
    "mask = y_raw < 2\n",
    "X = X_raw[mask]\n",
    "y = y_raw[mask]\n",
    "\n",
    "# Map labels {0,1} to {-1,1}\n",
    "y = np.where(y == 0, -1, 1).reshape(-1, 1)\n",
    "\n",
    "# Add intercept term\n",
    "m, n = X.shape\n",
    "Xb = np.hstack([np.ones((m, 1)), X])\n",
    "\n",
    "print(f\"Perceptron dataset: {m} examples, {n} features (plus bias).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1aa72",
   "metadata": {},
   "source": [
    "### Perceptron Update Rule\n",
    "\n",
    "The perceptron seeks a weight vector \\(\\theta\\) such that \\(\\text{sign}(\\theta^T \\tilde{x}) = y\\).  The\n",
    "algorithm initializes \\(\\theta\\) and iteratively scans through the dataset, updating \\(\\theta\\) whenever\n",
    "a misclassification occurs.  For an example \\((\\tilde{x}^{(i)}, y^{(i)})\\), the update is:\n",
    "\n",
    "\\[\n",
    "\\text{if } y^{(i)} (\\theta^T \\tilde{x}^{(i)}) \\le 0:\\quad \\theta := \\theta + y^{(i)} \\tilde{x}^{(i)}.\n",
    "\\]\n",
    "\n",
    "For linearly separable data, the perceptron converges in a finite number of mistakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_train(Xb: np.ndarray, y: np.ndarray, epochs: int = 50) -> tuple[np.ndarray, list]:\n",
    "    \"\"\"Train perceptron weights via the perceptron algorithm.\n",
    "\n",
    "    Returns:\n",
    "        theta: learned weight vector\n",
    "        mistakes_history: list recording cumulative mistakes per epoch\n",
    "    \"\"\"\n",
    "    theta = np.zeros((Xb.shape[1], 1))\n",
    "    mistakes_history = []\n",
    "    total_mistakes = 0\n",
    "    for epoch in range(epochs):\n",
    "        mistakes = 0\n",
    "        for i in range(Xb.shape[0]):\n",
    "            xi = Xb[i:i+1]  # shape (1, n+1)\n",
    "            yi = y[i, 0]\n",
    "            if yi * float(xi @ theta) <= 0:\n",
    "                theta += (yi * xi).T\n",
    "                mistakes += 1\n",
    "        total_mistakes += mistakes\n",
    "        mistakes_history.append(total_mistakes)\n",
    "    return theta, mistakes_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685a058",
   "metadata": {},
   "source": [
    "### Training the Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e04b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "theta_perc, mistakes_history = perceptron_train(Xb, y, epochs=epochs)\n",
    "\n",
    "print(f\"Total mistakes made: {mistakes_history[-1]}\")\n",
    "\n",
    "# Plot cumulative mistakes over epochs\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(mistakes_history, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cumulative Mistakes\")\n",
    "plt.title(\"Perceptron Training Progress\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d96f6",
   "metadata": {},
   "source": [
    "### Decision Boundary Visualization\n",
    "\n",
    "Because we use two features, we can visualize the separating hyperplane in the feature\n",
    "space.  The learned decision boundary satisfies \\(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = 0\\).\n",
    "\n",
    "Compute decision boundary line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73bcfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0, theta_1, theta_2 = theta_perc.flatten()\n",
    "x_vals = np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 200)\n",
    "if abs(theta_2) > 1e-6:\n",
    "    y_vals = - (theta_0 + theta_1 * x_vals) / theta_2\n",
    "else:\n",
    "    y_vals = np.zeros_like(x_vals)\n",
    "\n",
    "# Plot data points and decision boundary\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X[y[:, 0] == -1][:, 0], X[y[:, 0] == -1][:, 1], label=\"Class -1\", alpha=0.6)\n",
    "plt.scatter(X[y[:, 0] == 1][:, 0], X[y[:, 0] == 1][:, 1], label=\"Class +1\", alpha=0.6)\n",
    "plt.plot(x_vals, y_vals, 'r-', label=\"Perceptron boundary\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Perceptron Decision Boundary\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e687cc02",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Kernel Perceptron**: Extend the perceptron to non‑linearly separable data using the kernel\n",
    "   trick.  Implement a kernel perceptron with a polynomial or RBF kernel.\n",
    "2. **Learning Rate**: Introduce a learning rate \\(\\alpha\\) in the update rule: \\(\\theta := \\theta + \\alpha y^{(i)} \\tilde{x}^{(i)}\\).\n",
    "   Experiment with different \\(\\alpha\\) values and observe the convergence behaviour.\n",
    "3. **Feature Scaling**: Try normalizing the features before training.  Does it change the\n",
    "   number of mistakes or convergence speed?\n",
    "4. **Comparison with Logistic Regression**: Train logistic regression on the same dataset.\n",
    "   Compare decision boundaries and classification accuracy.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- The perceptron algorithm is an online learning method that finds a linear separator\n",
    "  for linearly separable data by updating weights whenever a misclassification occurs.\n",
    "- It converges in a finite number of updates if a separating hyperplane exists.  The\n",
    "  convergence proof shows the number of mistakes is bounded by the margin and the\n",
    "  radius of the data.\n",
    "- Unlike logistic regression, the perceptron uses a discrete step function and does\n",
    "  not output probabilities; it is sensitive to feature scaling but easy to implement.\n",
    "- Introducing kernels allows the perceptron to classify non‑linearly separable data\n",
    "  by implicitly mapping inputs into higher‑dimensional feature spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
