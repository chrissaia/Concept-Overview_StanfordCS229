{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc807053",
   "metadata": {},
   "source": [
    "# Lesson 6 — Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines are powerful margin‑based classifiers.  In their linear form\n",
    "they seek a hyperplane that maximizes the margin between classes while allowing for\n",
    "some misclassifications controlled by a regularization term.  In this notebook we\n",
    "implement a **linear soft‑margin SVM** using a subgradient descent approach on the\n",
    "hinge loss.  We use a binary dataset and compare the resulting classifier to\n",
    "logistic regression.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Data preparation**: load and preprocess a binary classification dataset.\n",
    "- **Hinge loss & gradient**: define the SVM objective and compute subgradients.\n",
    "- **Training via subgradient descent**: update rule with regularization.\n",
    "- **Model evaluation**: calculate accuracy and visualize projections.\n",
    "- **Exercises & interview summary**: deeper exploration and key takeaways.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977f150",
   "metadata": {},
   "source": [
    "### Imports & Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8eceecee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:41:59.001908Z",
     "start_time": "2026-02-04T15:41:58.970360Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # datasets only\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the breast cancer dataset again (two classes)\n",
    "X_raw, y_raw = datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Standardize features\n",
    "X = (X_raw - X_raw.mean(axis=0)) / X_raw.std(axis=0)\n",
    "\n",
    "# Map labels to {-1, +1}\n",
    "y = np.where(y_raw == 0, -1, 1).reshape(-1, 1)\n",
    "\n",
    "# Add intercept term\n",
    "m, n = X.shape\n",
    "Xb = np.hstack([np.ones((m, 1)), X])\n",
    "\n",
    "print(f\"SVM dataset: {m} examples, {n} features (+ bias)\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM dataset: 569 examples, 30 features (+ bias)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "5b24e587",
   "metadata": {},
   "source": [
    "### Hinge Loss and Subgradient\n",
    "\n",
    "For a linear SVM with parameters \\(\\theta\\) (including bias), the objective we minimize is\n",
    "\n",
    "\\[\n",
    "L(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\max\\bigl(0, 1 - y^{(i)} (\\theta^T \\tilde{x}^{(i)})\\bigr)\n",
    "+ \\frac{\\lambda}{2} \\lVert \\theta \\rVert^2,\n",
    "\\]\n",
    "\n",
    "where \\(\\lambda\\) controls the trade‑off between margin maximization and slack penalties.  The\n",
    "subgradient of the hinge loss is\n",
    "\n",
    "\\[\n",
    "\\partial L = - \\frac{1}{m} \\sum_{i: y^{(i)} \\theta^T \\tilde{x}^{(i)} < 1} y^{(i)} \\tilde{x}^{(i)} + \\lambda \\theta.\n",
    "\\]\n",
    "\n",
    "We will implement a basic subgradient descent loop to minimize this objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9dd74bfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:41:59.014010Z",
     "start_time": "2026-02-04T15:41:59.003644Z"
    }
   },
   "source": [
    "def svm_subgradient(Xb: np.ndarray, y: np.ndarray, theta: np.ndarray, lambda_: float) -> np.ndarray:\n",
    "    \"\"\"Compute the subgradient of the SVM objective.\"\"\"\n",
    "    m = Xb.shape[0]\n",
    "    subgrad = np.zeros_like(theta)\n",
    "    # Accumulate subgradients for examples that violate margin\n",
    "    margins = y * (Xb @ theta)\n",
    "    violating = margins < 1\n",
    "    if violating.any():\n",
    "        subgrad = - (Xb[violating.flatten()].T @ y[violating]) / m\n",
    "    # Add regularization term\n",
    "    subgrad += lambda_ * theta\n",
    "    return subgrad\n",
    "\n",
    "def train_svm(Xb: np.ndarray, y: np.ndarray, lambda_: float = 0.01, lr: float = 0.01, epochs: int = 1000) -> tuple[np.ndarray, list]:\n",
    "    \"\"\"Train a linear soft‑margin SVM using subgradient descent.\"\"\"\n",
    "    theta = np.zeros((Xb.shape[1], 1))\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        # Compute objective value (for monitoring)\n",
    "        margins = y * (Xb @ theta)\n",
    "        hinge_losses = np.maximum(0, 1 - margins)\n",
    "        loss = hinge_losses.mean() + 0.5 * lambda_ * float((theta.T @ theta))\n",
    "        history.append(loss)\n",
    "        # Subgradient\n",
    "        grad = svm_subgradient(Xb, y, theta, lambda_)\n",
    "        theta -= lr * grad\n",
    "    return theta, history\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "d5160388",
   "metadata": {},
   "source": [
    "### Training the SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cbdeed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T15:41:59.066756Z",
     "start_time": "2026-02-04T15:41:59.014593Z"
    }
   },
   "source": [
    "lambda_ = 0.01\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "\n",
    "theta_svm, history_svm = train_svm(Xb, y, lambda_=lambda_, lr=learning_rate, epochs=epochs)\n",
    "\n",
    "print(f\"Final SVM objective value: {history_svm[-1]:.4f}\")\n",
    "\n",
    "# Plot objective over epochs\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history_svm)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Objective Value\")\n",
    "plt.title(\"Linear SVM Convergence\")\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only 0-dimensional arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m learning_rate = \u001B[32m0.01\u001B[39m\n\u001B[32m      3\u001B[39m epochs = \u001B[32m500\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m theta_svm, history_svm = \u001B[43mtrain_svm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlambda_\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlambda_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFinal SVM objective value: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhistory_svm[-\u001B[32m1\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Plot objective over epochs\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 22\u001B[39m, in \u001B[36mtrain_svm\u001B[39m\u001B[34m(Xb, y, lambda_, lr, epochs)\u001B[39m\n\u001B[32m     20\u001B[39m margins = y * (Xb @ theta)\n\u001B[32m     21\u001B[39m hinge_losses = np.maximum(\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m - margins)\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m loss = hinge_losses.mean() + \u001B[32m0.5\u001B[39m * lambda_ * \u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtheta\u001B[49m\u001B[43m.\u001B[49m\u001B[43mT\u001B[49m\u001B[43m \u001B[49m\u001B[43m@\u001B[49m\u001B[43m \u001B[49m\u001B[43mtheta\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m history.append(loss)\n\u001B[32m     24\u001B[39m \u001B[38;5;66;03m# Subgradient\u001B[39;00m\n",
      "\u001B[31mTypeError\u001B[39m: only 0-dimensional arrays can be converted to Python scalars"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "e40581a8",
   "metadata": {},
   "source": [
    "### Model Evaluation & Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f459ceb7",
   "metadata": {},
   "source": [
    "def predict_svm(Xb: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Predict class labels using the trained SVM.\"\"\"\n",
    "    return np.where((Xb @ theta) >= 0, 1, -1)\n",
    "\n",
    "# Training accuracy\n",
    "svm_preds = predict_svm(Xb, theta_svm)\n",
    "svm_accuracy = (svm_preds == y).mean()\n",
    "print(f\"Training accuracy: {svm_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Visualize decision boundary using two principal components\n",
    "U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "X_pc = X @ Vt.T[:, :2]\n",
    "X_pc_b = np.hstack([np.ones((m, 1)), X_pc])\n",
    "theta_pc, _ = train_svm(X_pc_b, y, lambda_=lambda_, lr=learning_rate, epochs=epochs)\n",
    "\n",
    "# Compute line for decision boundary\n",
    "theta0, theta1, theta2 = theta_pc.flatten()\n",
    "x_vals = np.linspace(X_pc[:, 0].min() - 1, X_pc[:, 0].max() + 1, 200)\n",
    "if abs(theta2) > 1e-6:\n",
    "    y_vals = - (theta0 + theta1 * x_vals) / theta2\n",
    "else:\n",
    "    y_vals = np.zeros_like(x_vals)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X_pc[y.flatten() == -1][:, 0], X_pc[y.flatten() == -1][:, 1], alpha=0.5, label=\"Class -1\")\n",
    "plt.scatter(X_pc[y.flatten() == 1][:, 0], X_pc[y.flatten() == 1][:, 1], alpha=0.5, label=\"Class +1\")\n",
    "plt.plot(x_vals, y_vals, 'r-', label=\"SVM decision boundary (PC space)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Linear SVM Decision Boundary\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f9f2a1c8",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Parameter Tuning**: Experiment with different values of \\(\\lambda\\) and learning rate.  How does the\n",
    "   margin and training accuracy change?\n",
    "2. **Pegasos Algorithm**: Implement the Pegasos stochastic subgradient method for SVMs.  Compare its\n",
    "   convergence to the batch subgradient descent used here.\n",
    "3. **Nonlinear SVM**: Implement a kernelized SVM using the kernel trick (e.g., RBF kernel).  Use\n",
    "   scikit‑learn's SVM implementation for comparison, but derive the dual formulation yourself.\n",
    "4. **Comparison to Logistic Regression**: Plot the decision boundary of a logistic regression classifier\n",
    "   trained on the same dataset in the PC space.  Discuss similarities and differences.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- Linear SVMs optimize a margin‑based objective: minimize hinge loss plus an\n",
    "  \\(\\ell_2\\) regularization term.  The hinge loss penalizes misclassified and\n",
    "  insufficiently separated points.\n",
    "- The subgradient of the objective consists of a term involving only examples that violate\n",
    "  the margin and a regularization term proportional to \\(\\theta\\).\n",
    "- Soft‑margin SVMs allow some misclassifications through the slack variables controlled\n",
    "  by the regularization parameter \\(\\lambda\\).\n",
    "- Subgradient descent provides a simple way to train linear SVMs.  The Pegasos algorithm\n",
    "  further improves efficiency by using stochastic updates.\n",
    "- SVMs can be extended to nonlinear decision boundaries via kernel functions, leading to\n",
    "  nonlinear SVMs that implicitly operate in high‑dimensional feature spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
