{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc79c9c5",
   "metadata": {},
   "source": [
    "# Lesson 16 — Reinforcement Learning: Q‑Learning\n",
    "\n",
    "Q‑learning is a model‑free reinforcement learning algorithm that learns the value\n",
    "of taking a particular action in a given state, \\(Q(s,a)\\), without requiring\n",
    "knowledge of the environment's transition dynamics.  In this notebook we implement\n",
    "Q‑learning on a simple gridworld.  The agent learns optimal behaviour through\n",
    "exploration and exploitation.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Environment setup**: gridworld with rewards and terminal states.\n",
    "- **Q‑learning algorithm**: update rule and epsilon‑greedy exploration.\n",
    "- **Training loop**: run episodes and update the Q‑table.\n",
    "- **Visualization**: plot episode rewards and derived policy.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ff0ac",
   "metadata": {},
   "source": [
    "### Imports & Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1dfaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Gridworld parameters (reuse from Lesson 15)\n",
    "grid_size = (5, 5)\n",
    "goal_state = (4, 4)\n",
    "pit_state = (2, 2)\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right\n",
    "\n",
    "def step(state, action):\n",
    "    \"\"\"Take an action and return next state and reward.\"\"\"\n",
    "    r, c = state\n",
    "    dr, dc = action\n",
    "    r_new = min(max(r + dr, 0), grid_size[0] - 1)\n",
    "    c_new = min(max(c + dc, 0), grid_size[1] - 1)\n",
    "    next_state = (r_new, c_new)\n",
    "    if next_state == pit_state:\n",
    "        return next_state, -10\n",
    "    elif next_state == goal_state:\n",
    "        return next_state, 0\n",
    "    else:\n",
    "        return next_state, -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5c1b3",
   "metadata": {},
   "source": [
    "### Q‑Learning Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(grid_size, episodes=500, alpha=0.1, gamma=0.9, epsilon=0.2):\n",
    "    \"\"\"Run Q‑learning on the gridworld and return the learned Q‑values and reward history.\"\"\"\n",
    "    Q = np.zeros((*grid_size, len(actions)))\n",
    "    reward_history = []\n",
    "    for ep in range(episodes):\n",
    "        state = (0, 0)\n",
    "        total_reward = 0\n",
    "        while state != goal_state and state != pit_state:\n",
    "            # Epsilon‑greedy action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                action_idx = np.random.randint(len(actions))\n",
    "            else:\n",
    "                action_idx = np.argmax(Q[state])\n",
    "            action = actions[action_idx]\n",
    "            next_state, reward = step(state, action)\n",
    "            total_reward += reward\n",
    "            # Q‑update\n",
    "            best_next = np.max(Q[next_state])\n",
    "            Q[state + (action_idx,)] += alpha * (reward + gamma * best_next - Q[state + (action_idx,)])\n",
    "            state = next_state\n",
    "        reward_history.append(total_reward)\n",
    "    return Q, reward_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3454e2",
   "metadata": {},
   "source": [
    "### Training the Agent and Visualizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ac83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 200\n",
    "Q, rewards = q_learning(grid_size, episodes=episodes, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "\n",
    "print(f\"Average reward over episodes: {np.mean(rewards):.2f}\")\n",
    "\n",
    "# Plot episode rewards\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Q‑Learning Training Rewards\")\n",
    "plt.show()\n",
    "\n",
    "# Derive policy from Q values\n",
    "policy = np.zeros((*grid_size, len(actions)))\n",
    "for r in range(grid_size[0]):\n",
    "    for c in range(grid_size[1]):\n",
    "        if (r, c) == goal_state:\n",
    "            continue\n",
    "        best_action = np.argmax(Q[r, c])\n",
    "        policy[r, c, best_action] = 1\n",
    "\n",
    "# Visualize policy arrows\n",
    "plt.figure(figsize=(5, 4))\n",
    "value_function = np.max(Q, axis=2)\n",
    "plt.imshow(value_function, cmap='coolwarm')\n",
    "for r in range(grid_size[0]):\n",
    "    for c in range(grid_size[1]):\n",
    "        if (r, c) == goal_state:\n",
    "            continue\n",
    "        a_idx = np.argmax(policy[r, c])\n",
    "        dr, dc = actions[a_idx]\n",
    "        plt.arrow(c, r, 0.3 * dc, -0.3 * dr, head_width=0.2, head_length=0.2, fc='k', ec='k')\n",
    "plt.scatter([pit_state[1]], [pit_state[0]], marker='x', color='black', s=100, label=\"Pit\")\n",
    "plt.scatter([goal_state[1]], [goal_state[0]], marker='*', color='gold', s=150, label=\"Goal\")\n",
    "plt.title(\"Derived Policy from Q‑Values\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d1b11",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Exploration Rate**: Experiment with different epsilon schedules (e.g., decay over time).\n",
    "   How does exploration affect learning speed and quality of the learned policy?\n",
    "2. **Learning Rate**: Adjust \\(\\alpha\\) and observe its effect on convergence and stability.\n",
    "3. **Larger Environment**: Scale up the gridworld, add more obstacles and varied rewards.\n",
    "   Does Q‑learning still converge within a reasonable number of episodes?\n",
    "4. **Function Approximation**: Replace the Q‑table with a neural network approximator and\n",
    "   implement Deep Q‑Learning (DQN) on a small environment.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- Q‑learning learns the action‑value function \\(Q(s,a)\\) by iteratively updating estimates\n",
    "  based on observed rewards and estimated value of the next state.  It does not\n",
    "  require a model of the environment's transitions.\n",
    "- The update rule is: \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\bigl[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\bigr]\\).\n",
    "- Exploration–exploitation trade‑off is typically handled via epsilon‑greedy\n",
    "  policies that choose a random action with probability \\(\\epsilon\\) and the best\n",
    "  known action otherwise.\n",
    "- The learned policy is derived by selecting the action with the maximum Q‑value in\n",
    "  each state.  Visualization of Q‑values as a heatmap helps interpret the agent's\n",
    "  expectations.\n",
    "- Q‑learning can be extended to continuous state spaces and high‑dimensional\n",
    "  problems through function approximation (e.g., neural networks in Deep Q‑Learning)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
