{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc784ebe",
   "metadata": {},
   "source": [
    "# Lesson 17 — Temporal Difference Learning with Function Approximation\n",
    "\n",
    "Temporal Difference (TD) learning combines ideas from Monte Carlo and dynamic\n",
    "programming methods to learn value functions from experience.  In this lesson we\n",
    "implement TD(0) with linear function approximation on a simple random walk\n",
    "environment.  We illustrate how the value estimates converge to the true values\n",
    "over episodes.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Environment**: random walk with terminal states.\n",
    "- **True value function**: analytical solution for comparison.\n",
    "- **Feature representation**: linear basis functions for states.\n",
    "- **TD(0) algorithm**: update rule for weight vector.\n",
    "- **Learning curve**: track root mean squared value error (RMSVE).\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40baca76",
   "metadata": {},
   "source": [
    "### Imports & Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd27734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Random walk environment: states 0 (terminal) through 5 (terminal), states 1-4 are nonterminal\n",
    "nonterminal_states = [1, 2, 3, 4]\n",
    "terminal_states = [0, 5]\n",
    "\n",
    "# True state values for nonterminal states (expected return starting from state i)\n",
    "true_values = {1: 1/6, 2: 2/6, 3: 3/6, 4: 4/6}\n",
    "\n",
    "# Transition probabilities: move left or right with equal probability\n",
    "def step(state):\n",
    "    if state in terminal_states:\n",
    "        return state, 0\n",
    "    move = np.random.choice([-1, 1])\n",
    "    next_state = state + move\n",
    "    # Reward is 1 only if we transition into state 5, else 0\n",
    "    reward = 1 if next_state == 5 else 0\n",
    "    return next_state, reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef47bbb",
   "metadata": {},
   "source": [
    "### Feature Representation & TD(0)\n",
    "\n",
    "We use a simple linear feature representation for each state: a binary feature for\n",
    "each nonterminal state.  The value estimate is \\(\\hat{v}(s; w) = w^T x(s)\\).  TD(0)\n",
    "update for weight vector \\(w\\) when transitioning from state \\(s\\) to \\(s'\\) with reward\n",
    "\\(r\\) is\n",
    "\n",
    "\\[\n",
    "w := w + \\alpha \\bigl[r + \\gamma \\hat{v}(s'; w) - \\hat{v}(s; w)\\bigr] x(s),\n",
    "\\]\n",
    "\n",
    "where \\(\\gamma\\) is the discount factor (1 for the random walk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bac381",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 5  # nonterminal states 1..4 -> we use index 1..4 inclusive\n",
    "feature_dim = 5\n",
    "\n",
    "# One‑hot feature representation: x(s) has 1 at index s and 0 elsewhere\n",
    "def features(state):\n",
    "    x = np.zeros(feature_dim)\n",
    "    if state in nonterminal_states:\n",
    "        x[state] = 1\n",
    "    return x\n",
    "\n",
    "def td_learning(alpha=0.1, episodes=100):\n",
    "    w = np.zeros(feature_dim)\n",
    "    rmsve_history = []\n",
    "    gamma = 1.0\n",
    "    for ep in range(episodes):\n",
    "        state = np.random.choice(nonterminal_states)\n",
    "        while state not in terminal_states:\n",
    "            x_s = features(state)\n",
    "            next_state, reward = step(state)\n",
    "            x_s_next = features(next_state)\n",
    "            # TD target\n",
    "            target = reward + gamma * np.dot(w, x_s_next)\n",
    "            prediction = np.dot(w, x_s)\n",
    "            # Update weights\n",
    "            w += alpha * (target - prediction) * x_s\n",
    "            state = next_state\n",
    "        # Compute RMSVE after each episode\n",
    "        squared_error = 0.0\n",
    "        for s in nonterminal_states:\n",
    "            v_hat = np.dot(w, features(s))\n",
    "            squared_error += (true_values[s] - v_hat) ** 2\n",
    "        rmsve = np.sqrt(squared_error / len(nonterminal_states))\n",
    "        rmsve_history.append(rmsve)\n",
    "    return w, rmsve_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150676c1",
   "metadata": {},
   "source": [
    "### Running TD(0) and Plotting Convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80360f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "episodes = 100\n",
    "weights, rmsve_hist = td_learning(alpha=alpha, episodes=episodes)\n",
    "\n",
    "print(\"Learned weights:\", weights)\n",
    "\n",
    "# Plot RMS value error over episodes\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(rmsve_hist)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"RMS Value Error\")\n",
    "plt.title(f\"TD(0) Convergence (alpha={alpha})\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af205c",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Step Size Sensitivity**: Run TD(0) with different learning rates \\(\\alpha\\) and compare\n",
    "   convergence speeds and stability.  Is there an optimal \\(\\alpha\\)?\n",
    "2. **Different Feature Representations**: Try using polynomial features (e.g., state index\n",
    "   normalized between 0 and 1) or coarse coding.  Compare the approximation\n",
    "   accuracy.\n",
    "3. **On‑Policy vs. Off‑Policy**: Implement TD learning for the random walk under\n",
    "   different behaviour policies and targets (e.g., importance sampling).\n",
    "4. **TD(λ)**: Extend the algorithm to TD(\\(\\lambda\\)) with eligibility traces and study\n",
    "   its effect on bias and variance.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- TD learning updates value estimates based on bootstrapped targets, combining\n",
    "  Monte Carlo sampling with dynamic programming.  It can learn online from\n",
    "  experience without waiting for an episode to terminate.\n",
    "- With linear function approximation, the TD(0) update adjusts the weight vector\n",
    "  in the direction of the temporal difference error multiplied by the feature\n",
    "  vector: \\(w \\leftarrow w + \\alpha (r + \\gamma v(s') - v(s)) x(s)\\).\n",
    "- Choosing an appropriate step size \\(\\alpha\\) is crucial: too large leads to\n",
    "  divergence, while too small slows learning.\n",
    "- Feature representation determines the class of value functions that can be\n",
    "  approximated; richer features can reduce approximation error but may require\n",
    "  more data to train.\n",
    "- TD methods extend naturally to control via actor–critic and Q‑learning algorithms\n",
    "  by learning action values and policies simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
