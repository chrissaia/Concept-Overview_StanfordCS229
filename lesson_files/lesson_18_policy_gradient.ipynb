{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "456b4eb1",
   "metadata": {},
   "source": [
    "# Lesson 18 — Policy Gradient Methods: REINFORCE on a Multi‑Armed Bandit\n",
    "\n",
    "Policy gradient methods optimize a parametrized policy directly by ascending the\n",
    "gradient of expected returns.  In this lesson we implement the REINFORCE algorithm\n",
    "on a simple multi‑armed bandit problem.  Although bandits have no state dynamics,\n",
    "they provide a clear illustration of how to update policy parameters using sampled\n",
    "rewards.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Bandit environment**: define arms with different reward probabilities.\n",
    "- **Policy parameterization**: softmax over preferences for each arm.\n",
    "- **REINFORCE algorithm**: update preferences using sampled rewards.\n",
    "- **Learning curves**: track the probability of selecting the optimal arm.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8cde8",
   "metadata": {},
   "source": [
    "### Imports & Bandit Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e615728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define a 3‑armed bandit: each arm yields reward 1 with a different probability\n",
    "arm_probs = np.array([0.2, 0.5, 0.8])  # arm 2 is optimal\n",
    "n_arms = len(arm_probs)\n",
    "\n",
    "def pull_arm(arm):\n",
    "    return 1 if np.random.rand() < arm_probs[arm] else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6d8da",
   "metadata": {},
   "source": [
    "### Policy Parameterization and REINFORCE\n",
    "\n",
    "We maintain a vector of preferences \\(\\theta\\) over arms.  The policy \\(\\pi(a; \\theta)\\)\n",
    "is given by the softmax of preferences.  The REINFORCE update for a bandit is\n",
    "\n",
    "\\[\n",
    "\\theta \\leftarrow \\theta + \\alpha \\bigl(r - b\\bigr) \\nabla_\\theta \\log \\pi(a; \\theta),\n",
    "\\]\n",
    "\n",
    "where \\(r\\) is the obtained reward and \\(b\\) is a baseline to reduce variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(prefs: np.ndarray) -> np.ndarray:\n",
    "    z = prefs - prefs.max()\n",
    "    exp_p = np.exp(z)\n",
    "    return exp_p / exp_p.sum()\n",
    "\n",
    "def reinforce_bandit(arm_probs, episodes=2000, alpha=0.1, baseline=True):\n",
    "    n_arms = len(arm_probs)\n",
    "    prefs = np.zeros(n_arms)\n",
    "    avg_reward = 0.0  # baseline\n",
    "    optimal_arm = np.argmax(arm_probs)\n",
    "    optimal_probs = []\n",
    "    for ep in range(1, episodes + 1):\n",
    "        policy = softmax(prefs)\n",
    "        # Sample action\n",
    "        arm = np.random.choice(n_arms, p=policy)\n",
    "        reward = pull_arm(arm)\n",
    "        # Update baseline (running average of rewards)\n",
    "        avg_reward += (reward - avg_reward) / ep\n",
    "        # Gradient of log pi wrt prefs is (1 - pi(a_i)) for chosen arm i, -pi(j) for others\n",
    "        grad_log_pi = -policy\n",
    "        grad_log_pi[arm] += 1\n",
    "        # Use baseline to reduce variance if enabled\n",
    "        baseline_val = avg_reward if baseline else 0.0\n",
    "        prefs += alpha * (reward - baseline_val) * grad_log_pi\n",
    "        # Track probability of selecting optimal arm\n",
    "        optimal_probs.append(policy[optimal_arm])\n",
    "    return prefs, optimal_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f57027",
   "metadata": {},
   "source": [
    "### Running REINFORCE and Plotting Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e97f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "prefs, opt_prob_history = reinforce_bandit(arm_probs, episodes=episodes, alpha=0.1, baseline=True)\n",
    "\n",
    "print(\"Learned preferences:\", prefs)\n",
    "print(\"Learned policy (softmax of prefs):\", softmax(prefs))\n",
    "\n",
    "# Plot probability of choosing optimal arm over time\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(opt_prob_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Probability of choosing optimal arm\")\n",
    "plt.title(\"REINFORCE on a Multi‑Armed Bandit\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c58567",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Baselines**: Experiment with different baseline strategies: constant zero,\n",
    "   running average (as above), or state‑dependent baselines.  Compare variance and\n",
    "   convergence speed.\n",
    "2. **Non‑Stationary Bandits**: Change the reward probabilities over time and observe\n",
    "   how the policy adapts.  Implement a sliding‑window baseline to adapt.\n",
    "3. **Actor–Critic**: Extend this bandit example to an actor–critic method by\n",
    "   learning a separate value function (critic) to provide a baseline.\n",
    "4. **Full MDP**: Implement REINFORCE on a simple episodic MDP (e.g., gridworld)\n",
    "   where trajectories are longer and involve state transitions.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- Policy gradient methods parameterize the policy and directly adjust parameters to\n",
    "  maximize expected returns using gradient ascent.  They are particularly suited\n",
    "  for problems with continuous or large action spaces.\n",
    "- The REINFORCE algorithm uses sampled returns to estimate the gradient of the\n",
    "  expected reward with respect to policy parameters.  It updates preferences in\n",
    "  the direction that increases the probability of actions yielding higher rewards.\n",
    "- Variance reduction via baselines (e.g., subtracting an estimate of the average\n",
    "  reward) improves learning stability.  Actor–critic methods generalize this idea\n",
    "  by learning a value function alongside the policy.\n",
    "- In bandit problems, REINFORCE learns to favour arms with higher reward\n",
    "  probabilities.  In full MDPs, it can learn stochastic policies that balance\n",
    "  exploration and exploitation over trajectories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
