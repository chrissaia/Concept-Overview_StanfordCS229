{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e80d05f",
   "metadata": {},
   "source": [
    "# Lesson 13 — Expectation–Maximization for Gaussian Mixture Models\n",
    "\n",
    "The Expectation–Maximization (EM) algorithm is a general technique for maximum\n",
    "likelihood estimation in latent variable models.  In this notebook we implement EM\n",
    "for a **Gaussian mixture model (GMM)**.  We generate synthetic data from a mixture\n",
    "of Gaussians, then use EM to recover the mixture parameters and visualize the\n",
    "clustering.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Data generation**: sample from a known mixture of Gaussians.\n",
    "- **Initialization**: guess initial component means, covariances and mixing weights.\n",
    "- **E‑step**: compute responsibilities (posterior probabilities of components).\n",
    "- **M‑step**: update parameters using responsibilities.\n",
    "- **Log‑likelihood monitoring**: track convergence.\n",
    "- **Visualization**: show estimated clusters and compare to ground truth.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59292525",
   "metadata": {},
   "source": [
    "### Imports & Data Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ecf65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate synthetic data from 2 Gaussians\n",
    "n_samples = 400\n",
    "means_true = np.array([[0, 0], [5, 5]])\n",
    "cov_true = np.array([[[1.0, 0.3], [0.3, 1.0]], [[1.5, -0.2], [-0.2, 1.0]]])\n",
    "weights_true = np.array([0.4, 0.6])\n",
    "\n",
    "components = np.random.choice(len(weights_true), size=n_samples, p=weights_true)\n",
    "X = np.vstack([\n",
    "    np.random.multivariate_normal(means_true[k], cov_true[k]) for k in components\n",
    "])\n",
    "\n",
    "print(f\"Generated {n_samples} data points from a Gaussian mixture.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec8a36",
   "metadata": {},
   "source": [
    "### EM Algorithm for GMM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a36cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_gmm(X: np.ndarray, k: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Initialize mixture weights, means and covariances.\"\"\"\n",
    "    n, d = X.shape\n",
    "    weights = np.ones(k) / k\n",
    "    # Randomly choose k data points as initial means\n",
    "    indices = np.random.choice(n, k, replace=False)\n",
    "    means = X[indices].copy()\n",
    "    # Use identity matrices for initial covariances\n",
    "    covariances = np.array([np.eye(d) for _ in range(k)])\n",
    "    return weights, means, covariances\n",
    "\n",
    "def multivariate_normal_pdf(X: np.ndarray, mean: np.ndarray, cov: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute multivariate normal PDF for each row in X.\"\"\"\n",
    "    d = mean.shape[0]\n",
    "    cov = cov + 1e-6 * np.eye(d)\n",
    "    cov_inv = np.linalg.inv(cov)\n",
    "    det = np.linalg.det(cov)\n",
    "    norm_const = 1.0 / np.sqrt(((2 * np.pi) ** d) * det)\n",
    "    diff = X - mean\n",
    "    exponent = -0.5 * np.einsum(\"ij,jk,ik->i\", diff, cov_inv, diff)\n",
    "    return norm_const * np.exp(exponent)\n",
    "\n",
    "def e_step(X: np.ndarray, weights: np.ndarray, means: np.ndarray, covs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Expectation step: compute responsibilities.\"\"\"\n",
    "    n = X.shape[0]\n",
    "    k = weights.shape[0]\n",
    "    resp = np.zeros((n, k))\n",
    "    for j in range(k):\n",
    "        resp[:, j] = weights[j] * multivariate_normal_pdf(X, means[j], covs[j])\n",
    "    # Normalize responsibilities\n",
    "    resp_sum = resp.sum(axis=1, keepdims=True)\n",
    "    resp = resp / resp_sum\n",
    "    return resp\n",
    "\n",
    "def m_step(X: np.ndarray, resp: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Maximization step: update weights, means and covariances.\"\"\"\n",
    "    n, d = X.shape\n",
    "    k = resp.shape[1]\n",
    "    nk = resp.sum(axis=0)  # effective number of points per component\n",
    "    weights = nk / n\n",
    "    means = (resp.T @ X) / nk[:, None]\n",
    "    covs = np.zeros((k, d, d))\n",
    "    for j in range(k):\n",
    "        diff = X - means[j]\n",
    "        covs[j] = (resp[:, j][:, None] * diff).T @ diff / nk[j]\n",
    "        covs[j] += 1e-6 * np.eye(d)  # add small diagonal for stability\n",
    "    return weights, means, covs\n",
    "\n",
    "def log_likelihood(X: np.ndarray, weights: np.ndarray, means: np.ndarray, covs: np.ndarray) -> float:\n",
    "    n = X.shape[0]\n",
    "    k = weights.shape[0]\n",
    "    ll = 0\n",
    "    for i in range(n):\n",
    "        prob = 0\n",
    "        for j in range(k):\n",
    "            prob += weights[j] * multivariate_normal_pdf(X[i:i+1], means[j], covs[j])[0]\n",
    "        ll += np.log(prob + 1e-15)\n",
    "    return ll\n",
    "\n",
    "def em_gmm(X: np.ndarray, k: int, max_iters: int = 100, tol: float = 1e-4) -> tuple[np.ndarray, np.ndarray, np.ndarray, list]:\n",
    "    \"\"\"Run EM to fit a Gaussian mixture model.\"\"\"\n",
    "    weights, means, covs = initialize_gmm(X, k)\n",
    "    ll_history = []\n",
    "    for iteration in range(max_iters):\n",
    "        # E‑step\n",
    "        resp = e_step(X, weights, means, covs)\n",
    "        # M‑step\n",
    "        weights, means, covs = m_step(X, resp)\n",
    "        # Log‑likelihood\n",
    "        ll = log_likelihood(X, weights, means, covs)\n",
    "        ll_history.append(ll)\n",
    "        # Check convergence\n",
    "        if iteration > 0 and abs(ll_history[-1] - ll_history[-2]) < tol:\n",
    "            break\n",
    "    return weights, means, covs, ll_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f46fd76",
   "metadata": {},
   "source": [
    "### Running EM and Visualizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1201989",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "weights_est, means_est, covs_est, ll_history = em_gmm(X, k)\n",
    "\n",
    "print(f\"EM converged in {len(ll_history)} iterations.\")\n",
    "print(\"Estimated weights:\", weights_est)\n",
    "print(\"Estimated means:\\n\", means_est)\n",
    "\n",
    "# Plot log‑likelihood\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(ll_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Log‑Likelihood\")\n",
    "plt.title(\"EM Log‑Likelihood for GMM\")\n",
    "plt.show()\n",
    "\n",
    "# Plot data with estimated means and covariances\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5, label=\"Data\")\n",
    "for j in range(k):\n",
    "    eigvals, eigvecs = np.linalg.eigh(covs_est[j])\n",
    "    order = eigvals.argsort()[::-1]\n",
    "    eigvals = eigvals[order]\n",
    "    eigvecs = eigvecs[:, order]\n",
    "    angle = np.degrees(np.arctan2(eigvecs[1, 0], eigvecs[0, 0]))\n",
    "    width, height = 2 * np.sqrt(eigvals)\n",
    "    ell = plt.matplotlib.patches.Ellipse(xy=means_est[j], width=width, height=height, angle=angle,\n",
    "                                         edgecolor='red', fc='None', lw=2, label=f\"Component {j}\")\n",
    "    plt.gca().add_patch(ell)\n",
    "plt.scatter(means_est[:, 0], means_est[:, 1], color='red', marker='x', s=100, label=\"Estimated means\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title(\"Estimated Gaussian Components\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085a3e8",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Number of Components**: Fit GMMs with different numbers of components and use the\n",
    "   Bayesian Information Criterion (BIC) to select k.\n",
    "2. **Initialization Methods**: Try random initialization or k‑means based initialization\n",
    "   for EM.  Compare convergence speed and parameter estimates.\n",
    "3. **Covariance Constraints**: Modify the algorithm to restrict covariances to be\n",
    "   diagonal (spherical) or tied across components.  Evaluate on high‑dimensional data.\n",
    "4. **Soft Clustering**: Use the responsibilities to compute soft cluster memberships\n",
    "   and compare to hard assignments (argmax of responsibilities).\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- The EM algorithm alternates between computing the expected latent variable\n",
    "  assignments (E‑step) and maximizing the expected complete‑data log‑likelihood with\n",
    "  respect to parameters (M‑step).  It monotonically increases the data likelihood.\n",
    "- In a Gaussian mixture model, the E‑step computes responsibilities proportional to\n",
    "  the weighted Gaussian densities.  The M‑step updates mixture weights, means and\n",
    "  covariances using these responsibilities.\n",
    "- Initialization matters; poor initial parameters can lead to local optima.  Multiple\n",
    "  restarts or k‑means initialization often improve results.\n",
    "- GMMs model arbitrary cluster shapes with full covariance matrices, unlike k‑means\n",
    "  which assumes spherical clusters.  Model selection criteria (e.g., BIC) help choose\n",
    "  the number of components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
