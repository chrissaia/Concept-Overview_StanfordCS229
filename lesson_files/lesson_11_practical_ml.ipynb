{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a71eee2",
   "metadata": {},
   "source": [
    "# Lesson 11 — Practical Machine Learning Advice\n",
    "\n",
    "Machine learning practitioners often face challenges beyond implementing algorithms.\n",
    "Proper data splitting, feature scaling, regularization and debugging are crucial\n",
    "for building robust models.  In this notebook we illustrate some practical\n",
    "considerations using logistic regression on the breast cancer dataset.  We explore\n",
    "learning curves, the effect of regularization, and provide guidelines for\n",
    "diagnosing issues when a model underperforms.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Data splitting**: training, validation and test sets.\n",
    "- **Learning curves**: evaluate performance vs. training set size.\n",
    "- **Regularization**: observe impact of \\(\\ell_2\\) regularization on logistic regression.\n",
    "- **Error analysis**: inspect misclassified examples.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f486b",
   "metadata": {},
   "source": [
    "### Imports & Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbdf4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # datasets only\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load breast cancer dataset and standardize\n",
    "X_raw, y_raw = datasets.load_breast_cancer(return_X_y=True)\n",
    "X = (X_raw - X_raw.mean(axis=0)) / X_raw.std(axis=0)\n",
    "y = y_raw.reshape(-1, 1)\n",
    "\n",
    "def split_train_val_test(X: np.ndarray, y: np.ndarray, seed: int = 42) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = rng.permutation(X.shape[0])\n",
    "    test_size = int(0.2 * X.shape[0])\n",
    "    val_size = int(0.2 * X.shape[0])\n",
    "    train_size = X.shape[0] - test_size - val_size\n",
    "    train_idx = indices[:train_size]\n",
    "    val_idx = indices[train_size:train_size + val_size]\n",
    "    test_idx = indices[train_size + val_size:]\n",
    "    return X[train_idx], X[val_idx], X[test_idx], y[train_idx], y[val_idx], y[test_idx]\n",
    "\n",
    "# Split into train (60%), validation (20%), test (20%)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_train_val_test(X, y)\n",
    "\n",
    "print(f\"Training examples: {X_train.shape[0]}, Validation: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb8813",
   "metadata": {},
   "source": [
    "### Logistic Regression with \\(\\ell_2\\) Regularization\n",
    "\n",
    "We reuse the logistic regression functions from Lesson 3 but add an \\(\\ell_2\\) penalty term.\n",
    "The regularized loss is\n",
    "\n",
    "\\[\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)}) \\log (1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m} \\lVert \\theta\\rVert^2.\n",
    "\\]\n",
    "\n",
    "The gradient becomes\n",
    "\n",
    "\\[\n",
    "\\nabla J(\\theta) = \\frac{1}{m} X_b^T (h - y) + \\frac{\\lambda}{m} \\theta.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def loss_reg(theta, Xb, y, lambda_):\n",
    "    m = Xb.shape[0]\n",
    "    h = sigmoid(Xb @ theta)\n",
    "    h = np.clip(h, 1e-15, 1 - 1e-15)\n",
    "    ce = - (y * np.log(h) + (1 - y) * np.log(1 - h)).mean()\n",
    "    reg = (lambda_ / (2 * m)) * float(theta.T @ theta)\n",
    "    return ce + reg\n",
    "\n",
    "def grad_reg(theta, Xb, y, lambda_):\n",
    "    m = Xb.shape[0]\n",
    "    h = sigmoid(Xb @ theta)\n",
    "    grad = (1 / m) * (Xb.T @ (h - y)) + (lambda_ / m) * theta\n",
    "    return grad\n",
    "\n",
    "def train_logistic_reg(Xb, y, lambda_=0.0, lr=0.1, epochs=1000):\n",
    "    theta = np.zeros((Xb.shape[1], 1))\n",
    "    for epoch in range(epochs):\n",
    "        grad = grad_reg(theta, Xb, y, lambda_)\n",
    "        theta -= lr * grad\n",
    "    return theta\n",
    "\n",
    "def predict(theta, Xb):\n",
    "    return (sigmoid(Xb @ theta) >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Add intercept terms\n",
    "X_train_b = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_val_b = np.hstack([np.ones((X_val.shape[0], 1)), X_val])\n",
    "X_test_b = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d1430",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "\n",
    "We train logistic regression on increasing fractions of the training data and compute\n",
    "training and validation accuracies.  This helps diagnose high bias (both errors high and\n",
    "similar) vs. high variance (training error low, validation error high).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "lambda_reg = 0.0  # no regularization for learning curve\n",
    "\n",
    "for frac in train_sizes:\n",
    "    m_frac = int(frac * X_train.shape[0])\n",
    "    idx = np.random.permutation(X_train.shape[0])[:m_frac]\n",
    "    Xb_sub = X_train_b[idx]\n",
    "    y_sub = y_train[idx]\n",
    "    theta = train_logistic_reg(Xb_sub, y_sub, lambda_=lambda_reg, lr=0.1, epochs=500)\n",
    "    train_pred = predict(theta, Xb_sub)\n",
    "    val_pred = predict(theta, X_val_b)\n",
    "    train_accs.append((train_pred == y_sub).mean())\n",
    "    val_accs.append((val_pred == y_val).mean())\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_sizes * 100, np.array(train_accs) * 100, label=\"Training accuracy\")\n",
    "plt.plot(train_sizes * 100, np.array(val_accs) * 100, label=\"Validation accuracy\")\n",
    "plt.xlabel(\"Training set size (%)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Learning Curves (Logistic Regression)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e341f",
   "metadata": {},
   "source": [
    "### Effect of Regularization\n",
    "\n",
    "We train logistic regression with different regularization strengths and evaluate on\n",
    "the validation set to see how \\(\\lambda\\) affects performance.  Too little\n",
    "regularization may overfit, while too much causes underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_values = [0.0, 0.01, 0.1, 1.0]\n",
    "val_accs_reg = []\n",
    "for lam in lambda_values:\n",
    "    theta = train_logistic_reg(X_train_b, y_train, lambda_=lam, lr=0.1, epochs=800)\n",
    "    val_pred = predict(theta, X_val_b)\n",
    "    val_acc = (val_pred == y_val).mean()\n",
    "    val_accs_reg.append(val_acc)\n",
    "    print(f\"Lambda={lam}, Validation accuracy: {val_acc * 100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.semilogx(lambda_values, np.array(val_accs_reg) * 100, marker='o')\n",
    "plt.xlabel(\"Lambda (log scale)\")\n",
    "plt.ylabel(\"Validation Accuracy (%)\")\n",
    "plt.title(\"Effect of \\u2113_2 Regularization on Accuracy\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6b20c",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "Inspect misclassified examples on the validation set.  Error analysis can reveal\n",
    "systematic patterns or mislabeled data.  Here we simply count misclassifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best = train_logistic_reg(X_train_b, y_train, lambda_=0.01, lr=0.1, epochs=800)\n",
    "val_pred_best = predict(theta_best, X_val_b)\n",
    "mis_idx = np.where(val_pred_best != y_val)[0]\n",
    "print(f\"Number of misclassified validation examples: {len(mis_idx)}\")\n",
    "\n",
    "# Print first few misclassified examples' indices and true/predicted labels\n",
    "for i in mis_idx[:5]:\n",
    "    print(f\"Index {i}, True label: {int(y_val[i])}, Predicted: {int(val_pred_best[i])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd2c3b",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Precision & Recall**: Compute precision, recall and F1 score for the logistic regression\n",
    "   classifier.  Discuss situations where accuracy is not sufficient.\n",
    "2. **Feature Scaling**: Experiment with non‑standardized features.  How does lack of scaling\n",
    "   affect convergence and performance of gradient descent?\n",
    "3. **Hyperparameter Tuning**: Implement grid search over learning rates and regularization\n",
    "   parameters using cross‑validation.  Plot validation accuracy as a function of hyperparameters.\n",
    "4. **Debugging**: Intentionally break the gradient computation and observe how the\n",
    "   learning curves change.  Use gradient checking to detect bugs.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- Split data into training, validation and test sets to detect overfitting and tune\n",
    "  hyperparameters.  The test set should only be used once at the end of model selection.\n",
    "- Learning curves show how performance scales with more data; parallel training and\n",
    "  validation curves suggest high bias (both low) or high variance (training high, validation low).\n",
    "- Regularization prevents overfitting by penalizing large weights.  The regularization\n",
    "  strength \\(\\lambda\\) must be tuned; too large causes underfitting.\n",
    "- Error analysis involves inspecting misclassified examples to identify patterns,\n",
    "  mislabeled data or feature inadequacies.\n",
    "- Proper feature scaling, debugging gradients and systematic hyperparameter tuning\n",
    "  are vital practical steps in building effective machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
