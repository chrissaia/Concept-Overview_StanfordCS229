{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49609678",
   "metadata": {},
   "source": [
    "# Lesson 8 — Decision Trees\n",
    "\n",
    "Decision trees recursively partition the feature space to form a tree of simple\n",
    "decision rules.  They are intuitive, handle nonlinear relationships and are the\n",
    "building blocks of powerful ensemble methods like random forests and boosting.\n",
    "In this notebook we implement a simple decision tree classifier from scratch\n",
    "using the Gini impurity criterion.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Data preparation**: load and preprocess a binary classification dataset.\n",
    "- **Impurity measures**: define Gini impurity and information gain.\n",
    "- **Tree building**: recursively split data to grow the tree.\n",
    "- **Prediction**: traverse the tree to classify new points.\n",
    "- **Visualization & evaluation**: plot decision boundaries for two features.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116fc474",
   "metadata": {},
   "source": [
    "### Imports & Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d77079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # datasets only\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the breast cancer dataset, restrict to two features for visualization\n",
    "X_raw, y_raw = datasets.load_breast_cancer(return_X_y=True)\n",
    "X = X_raw[:, :2]  # first two features\n",
    "y = y_raw.reshape(-1, 1)\n",
    "\n",
    "# Normalize features\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "print(f\"Dataset for decision tree: {X.shape[0]} examples, using 2 features.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36fe84",
   "metadata": {},
   "source": [
    "### Gini Impurity and Best Split\n",
    "\n",
    "The Gini impurity of a binary node with class proportion \\(p\\) is \\(2 p (1 - p)\\).\n",
    "To find the best split along a feature, we consider threshold candidates between\n",
    "consecutive sorted values and compute the weighted impurity of the child nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(labels: np.ndarray) -> float:\n",
    "    \"\"\"Compute Gini impurity for binary labels (0/1).\"\"\"\n",
    "    if len(labels) == 0:\n",
    "        return 0.0\n",
    "    p = labels.mean()\n",
    "    return 2 * p * (1 - p)\n",
    "\n",
    "def best_split(X: np.ndarray, y: np.ndarray) -> tuple[int, float, float]:\n",
    "    \"\"\"Find the best feature and threshold to split on using Gini impurity.\n",
    "\n",
    "    Returns:\n",
    "        best_feature_index, best_threshold, best_impurity\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    best_impurity = float('inf')\n",
    "    best_feature = None\n",
    "    best_thresh = None\n",
    "    for j in range(n):\n",
    "        # Sort examples by feature j\n",
    "        sorted_idx = X[:, j].argsort()\n",
    "        X_sorted = X[sorted_idx, j]\n",
    "        y_sorted = y[sorted_idx]\n",
    "        # Candidate thresholds: midpoints between unique values\n",
    "        for i in range(1, m):\n",
    "            if X_sorted[i] == X_sorted[i - 1]:\n",
    "                continue\n",
    "            thresh = 0.5 * (X_sorted[i] + X_sorted[i - 1])\n",
    "            left_labels = y_sorted[:i]\n",
    "            right_labels = y_sorted[i:]\n",
    "            impurity = (len(left_labels) * gini_impurity(left_labels) + len(right_labels) * gini_impurity(right_labels)) / m\n",
    "            if impurity < best_impurity:\n",
    "                best_impurity = impurity\n",
    "                best_feature = j\n",
    "                best_thresh = thresh\n",
    "    return best_feature, best_thresh, best_impurity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d2cddf",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "\n",
    "We build a binary tree recursively.  Each node stores the feature index and threshold for\n",
    "splitting, along with left and right child nodes.  Leaves store the predicted class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, prediction=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.prediction = prediction  # class label for leaf nodes\n",
    "\n",
    "def build_tree(X: np.ndarray, y: np.ndarray, depth: int = 0, max_depth: int = 3, min_samples_split: int = 2) -> TreeNode:\n",
    "    \"\"\"Recursively build a decision tree using Gini impurity.\"\"\"\n",
    "    # If all labels are the same or max depth reached, create a leaf\n",
    "    if len(set(y.flatten())) == 1 or depth >= max_depth or len(y) < min_samples_split:\n",
    "        # Predict majority class\n",
    "        prediction = 1 if y.mean() >= 0.5 else 0\n",
    "        return TreeNode(prediction=prediction)\n",
    "    # Find best split\n",
    "    feature, thresh, impurity = best_split(X, y)\n",
    "    if feature is None:\n",
    "        # Could not split (all feature values identical)\n",
    "        prediction = 1 if y.mean() >= 0.5 else 0\n",
    "        return TreeNode(prediction=prediction)\n",
    "    # Partition data\n",
    "    left_mask = X[:, feature] <= thresh\n",
    "    right_mask = ~left_mask\n",
    "    left_node = build_tree(X[left_mask], y[left_mask], depth + 1, max_depth, min_samples_split)\n",
    "    right_node = build_tree(X[right_mask], y[right_mask], depth + 1, max_depth, min_samples_split)\n",
    "    return TreeNode(feature=feature, threshold=thresh, left=left_node, right=right_node)\n",
    "\n",
    "def tree_predict(x: np.ndarray, node: TreeNode) -> int:\n",
    "    \"\"\"Predict class label for a single example using the decision tree.\"\"\"\n",
    "    if node.prediction is not None:\n",
    "        return node.prediction\n",
    "    if x[node.feature] <= node.threshold:\n",
    "        return tree_predict(x, node.left)\n",
    "    else:\n",
    "        return tree_predict(x, node.right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cdf954",
   "metadata": {},
   "source": [
    "### Training the Tree and Evaluating\n",
    "\n",
    "Build a shallow tree for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d49bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_root = build_tree(X, y, max_depth=3)\n",
    "\n",
    "# Predict on training data\n",
    "y_pred = np.array([tree_predict(x, tree_root) for x in X]).reshape(-1, 1)\n",
    "accuracy = (y_pred == y).mean()\n",
    "print(f\"Training accuracy of shallow decision tree: {accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f6fd68",
   "metadata": {},
   "source": [
    "### Visualization of Decision Boundaries\n",
    "\n",
    "We visualize the decision regions in the 2D feature space by evaluating the tree on a\n",
    "grid of points and plotting the resulting class predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87925f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "grid_pred = np.array([tree_predict(p, tree_root) for p in grid_points]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.contourf(xx, yy, grid_pred, levels=[-0.5, 0.5, 1.5], alpha=0.3, cmap='bwr')\n",
    "plt.scatter(X[y.flatten() == 0][:, 0], X[y.flatten() == 0][:, 1], label=\"Class 0\", alpha=0.6)\n",
    "plt.scatter(X[y.flatten() == 1][:, 0], X[y.flatten() == 1][:, 1], label=\"Class 1\", alpha=0.6)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Decision Tree Decision Regions (Depth 3)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85b9b07",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Max Depth Tuning**: Train trees with different maximum depths and plot training vs. validation\n",
    "   accuracy to observe overfitting.\n",
    "2. **Information Gain**: Implement a decision tree using information gain (entropy) instead of\n",
    "   Gini impurity and compare the resulting trees.\n",
    "3. **Ensembles**: Implement bagging (bootstrap aggregating) by training multiple shallow trees\n",
    "   on bootstrap samples and averaging their predictions.  Evaluate the ensemble's accuracy.\n",
    "4. **Continuous Splits**: Extend the tree to handle multi‑class problems and continuous splits\n",
    "   with more than two outcomes.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- Decision trees partition the feature space by recursively splitting on features that\n",
    "  maximize some purity criterion (e.g., Gini impurity or information gain).\n",
    "- A shallow tree may underfit, while a deep tree can overfit; controlling depth and\n",
    "  minimum samples per split mitigates overfitting.\n",
    "- Trees are interpretable: each path corresponds to a sequence of simple rules leading\n",
    "  to a prediction.\n",
    "- Ensembles like random forests and boosting use many trees to reduce variance and\n",
    "  improve performance over a single tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
