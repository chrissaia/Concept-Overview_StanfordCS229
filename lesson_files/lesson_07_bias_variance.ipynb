{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26fd16e",
   "metadata": {},
   "source": [
    "# Lesson 7 — Bias–Variance Trade‑off & Cross‑Validation\n",
    "\n",
    "Overfitting and underfitting are central themes in machine learning.  In this notebook\n",
    "we illustrate the **bias–variance trade‑off** by fitting polynomial regression models\n",
    "of varying degrees to noisy data.  We use cross‑validation to estimate the prediction\n",
    "error and discuss how model complexity affects bias and variance.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Synthetic data generation**: create a noisy nonlinear function.\n",
    "- **Polynomial feature construction**: map scalar inputs to polynomial basis.\n",
    "- **Model fitting**: fit models of different degrees via the normal equation.\n",
    "- **Training vs. validation error**: compute MSE on splits and plot.\n",
    "- **Visualization**: show model fits and error curves.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8160d",
   "metadata": {},
   "source": [
    "### Imports & Data Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe730ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "m = 200\n",
    "X_raw = np.random.uniform(-2, 2, size=(m, 1))\n",
    "# True function: sine wave\n",
    "def f_true(x):\n",
    "    return np.sin(np.pi * x)\n",
    "y_true = f_true(X_raw)\n",
    "# Add noise\n",
    "noise = 0.3 * np.random.randn(m, 1)\n",
    "y = y_true + noise\n",
    "\n",
    "print(f\"Generated {m} data points for bias–variance analysis.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f20b061",
   "metadata": {},
   "source": [
    "### Polynomial Feature Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de69730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_features(X: np.ndarray, degree: int) -> np.ndarray:\n",
    "    \"\"\"Map input X (m x 1) to polynomial features up to the given degree.\"\"\"\n",
    "    X_poly = np.ones((X.shape[0], degree + 1))\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly[:, d] = X.flatten() ** d\n",
    "    return X_poly\n",
    "\n",
    "def normal_equation(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute closed‑form solution to linear regression via normal equations.\"\"\"\n",
    "    return np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "def mse(y_hat: np.ndarray, y: np.ndarray) -> float:\n",
    "    return float(((y_hat - y) ** 2).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb0fb5",
   "metadata": {},
   "source": [
    "### Cross‑Validation and Error Curves\n",
    "\n",
    "Split data into training (60%), validation (20%), and test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95873c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(m)\n",
    "train_end = int(0.6 * m)\n",
    "val_end = int(0.8 * m)\n",
    "train_idx = indices[:train_end]\n",
    "val_idx = indices[train_end:val_end]\n",
    "test_idx = indices[val_end:]\n",
    "\n",
    "X_train, y_train = X_raw[train_idx], y[train_idx]\n",
    "X_val, y_val = X_raw[val_idx], y[val_idx]\n",
    "X_test, y_test = X_raw[test_idx], y[test_idx]\n",
    "\n",
    "# Fit models of increasing degree\n",
    "degrees = list(range(0, 11))  # 0 to 10\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "for d in degrees:\n",
    "    X_train_poly = poly_features(X_train, d)\n",
    "    theta = normal_equation(X_train_poly, y_train)\n",
    "    # Predictions\n",
    "    train_pred = X_train_poly @ theta\n",
    "    train_errors.append(mse(train_pred, y_train))\n",
    "    # Validation\n",
    "    X_val_poly = poly_features(X_val, d)\n",
    "    val_pred = X_val_poly @ theta\n",
    "    val_errors.append(mse(val_pred, y_val))\n",
    "\n",
    "# Plot training and validation error vs. degree\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(degrees, train_errors, label=\"Training error\")\n",
    "plt.plot(degrees, val_errors, label=\"Validation error\")\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Bias–Variance Trade‑off via Polynomial Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c996baf",
   "metadata": {},
   "source": [
    "### Visualizing Model Fits\n",
    "\n",
    "We'll visualize the fitted curves for a low‑degree model (underfitting), an\n",
    "intermediate model (good fit) and a high‑degree model (overfitting) using the\n",
    "full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992db294",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees_to_plot = [1, 3, 9]\n",
    "plt.figure(figsize=(12, 4))\n",
    "X_plot = np.linspace(-2, 2, 200).reshape(-1, 1)\n",
    "\n",
    "for i, d in enumerate(degrees_to_plot):\n",
    "    theta = normal_equation(poly_features(X_train, d), y_train)\n",
    "    y_plot = poly_features(X_plot, d) @ theta\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.scatter(X_train, y_train, alpha=0.3, label=\"Training data\")\n",
    "    plt.plot(X_plot, f_true(X_plot), 'g--', label=\"True function\")\n",
    "    plt.plot(X_plot, y_plot, 'r-', label=f\"Degree {d} fit\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(f\"Model degree {d}\")\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811318dc",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **K‑Fold Cross‑Validation**: Implement k‑fold cross‑validation (e.g., k=5) to select\n",
    "   the optimal polynomial degree based on validation error.  Compare to the simple\n",
    "   train/validation split used here.\n",
    "2. **Regularization**: Fit polynomial regression with \\(\\ell_2\\) regularization (ridge regression)\n",
    "   and explore how different regularization strengths affect the bias–variance trade‑off.\n",
    "3. **Different True Functions**: Repeat the experiment with a different underlying\n",
    "   function (e.g., exponential, piecewise).  Observe how model complexity interacts\n",
    "   with the true function.\n",
    "4. **Variance Decomposition**: Derive the bias and variance analytically for a simple\n",
    "   estimator (e.g., sample mean) and compare to empirical estimates.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- The **bias–variance trade‑off** captures the tension between underfitting and\n",
    "  overfitting: simple models have high bias but low variance, while complex models\n",
    "  have low bias but high variance.\n",
    "- Cross‑validation helps estimate prediction error by simulating unseen data.  The\n",
    "  validation error typically decreases then increases as model complexity grows,\n",
    "  illustrating the trade‑off.\n",
    "- Polynomial regression provides a simple playground for bias–variance experiments.\n",
    "  The normal equation gives the closed‑form solution, which can be regularized to\n",
    "  prevent overfitting.\n",
    "- Selecting model complexity based on validation error rather than training error\n",
    "  leads to better generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
