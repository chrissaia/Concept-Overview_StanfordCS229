{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f97bf48",
   "metadata": {},
   "source": [
    "# Lesson 2 — Linear Regression from Scratch\n",
    "\n",
    "In this notebook we will build a linear regression model from the ground up using only\n",
    "**NumPy**.  We follow the structure of CS229 to introduce notation, define the objective,\n",
    "derive the gradient descent algorithm, and build intuition through visualizations.  At the\n",
    "end you'll find exercises and interview‑style questions to reinforce understanding.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Introduction & Notation**: formalize the hypothesis, parameters and cost function.\n",
    "- **Data loading & preprocessing**: obtain a regression dataset and standardize features.\n",
    "- **Gradient Descent Implementation**: derive and implement batch gradient descent.\n",
    "- **Stochastic Gradient Descent**: implement SGD and compare convergence.\n",
    "- **Visualization**: plot the learned regressors on individual features.\n",
    "- **Exercises & Interview Q**: practice problems and summarization prompts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9a95a",
   "metadata": {},
   "source": [
    "### Introduction & Notation\n",
    "\n",
    "We consider a supervised learning problem with training examples \\(\\{(x^{(i)}, y^{(i)})\\}\\_{i=1}^m\\).  Each\n",
    "feature vector \\(x^{(i)} \\in \\mathbb{R}^n\\) is augmented with an intercept term to form \\(\\tilde{x}^{(i)} = [1, x_1^{(i)}, \\dots, x_n^{(i)}]^T\\).\n",
    "Our hypothesis family is linear:\n",
    "\n",
    "\\[\n",
    "h_\\theta(x) = \\theta^T \\tilde{x},\n",
    "\\]\n",
    "\n",
    "where \\(\\theta \\in \\mathbb{R}^{n+1}\\) collects the bias and weights.  The mean squared error (MSE) cost function\n",
    "over the dataset is\n",
    "\n",
    "\\[\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2.\n",
    "\\]\n",
    "\n",
    "Minimizing \\(J(\\theta)\\) with respect to \\(\\theta\\) yields the ordinary least squares solution.  We will derive\n",
    "the gradient and use gradient descent to find \\(\\theta\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6da29e",
   "metadata": {},
   "source": [
    "### Imports & Random Seed\n",
    "We'll use NumPy for array operations and Matplotlib for plotting.  We also import a toy\n",
    "dataset from scikit‑learn just to obtain a realistic regression problem.  Using `sklearn`\n",
    "to fetch data is allowed here, but we will **not** use its modeling APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # datasets only\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835f4ea",
   "metadata": {},
   "source": [
    "### Data Loading & Preprocessing\n",
    "\n",
    "For demonstration purposes we use the **diabetes** dataset from scikit‑learn, which consists of\n",
    "ten real‑valued features and a continuous target.  We standardize each feature to have\n",
    "zero mean and unit variance.  Then we add an intercept column of ones.\n",
    "\n",
    "Load the dataset (only the features and target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f417da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, y_raw = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Standardize features\n",
    "X = (X_raw - X_raw.mean(axis=0)) / X_raw.std(axis=0)\n",
    "y = y_raw.reshape(-1, 1)  # ensure column vector\n",
    "\n",
    "m, n = X.shape  # number of examples and features\n",
    "\n",
    "# Add intercept term\n",
    "Xb = np.hstack([np.ones((m, 1)), X])  # shape (m, n+1)\n",
    "\n",
    "print(f\"Loaded dataset with {m} examples and {n} features.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73521a",
   "metadata": {},
   "source": [
    "### Gradient Descent Derivation\n",
    "\n",
    "The partial derivative of the MSE cost with respect to \\(\\theta\\) is\n",
    "\n",
    "\\[\n",
    "\\nabla_\\theta J(\\theta) = \\frac{2}{m} X_b^T \\bigl( X_b \\theta - y \\bigr).\n",
    "\\]\n",
    "\n",
    "We initialize \\(\\theta\\) to zeros and update it via the batch gradient descent rule:\n",
    "\n",
    "\\[\n",
    "\\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta),\n",
    "\\]\n",
    "\n",
    "where \\(\\alpha\\) is the learning rate.  Below we implement the loss function, its gradient,\n",
    "and a training loop to optimize \\(\\theta\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(Xb: np.ndarray, y: np.ndarray, theta: np.ndarray) -> float:\n",
    "    \"\"\"Compute the mean squared error loss.\"\"\"\n",
    "    m = Xb.shape[0]\n",
    "    errors = Xb @ theta - y\n",
    "    return float((errors.T @ errors) / m)\n",
    "\n",
    "def mse_gradient(Xb: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute the gradient of the MSE loss w.r.t. theta.\"\"\"\n",
    "    m = Xb.shape[0]\n",
    "    return (2.0 / m) * Xb.T @ (Xb @ theta - y)\n",
    "\n",
    "def gradient_descent(Xb: np.ndarray, y: np.ndarray, lr: float = 0.1, epochs: int = 2000) -> tuple[np.ndarray, list]:\n",
    "    \"\"\"Perform batch gradient descent to minimize the MSE loss.\n",
    "\n",
    "    Returns:\n",
    "        theta (np.ndarray): optimized parameters\n",
    "        history (list): list of loss values at each epoch\n",
    "    \"\"\"\n",
    "    theta = np.zeros((Xb.shape[1], 1))\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = mse_loss(Xb, y, theta)\n",
    "        history.append(loss)\n",
    "        grad = mse_gradient(Xb, y, theta)\n",
    "        theta -= lr * grad\n",
    "    return theta, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e88105",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "We train the model using a moderate learning rate.  The convergence can be observed by\n",
    "plotting the loss over epochs.  In practice one might use more sophisticated learning\n",
    "rate schedules or optimize the number of epochs based on validation error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "epochs = 500\n",
    "\n",
    "theta_gd, history_gd = gradient_descent(Xb, y, lr=learning_rate, epochs=epochs)\n",
    "\n",
    "print(\"Optimized theta shape:\", theta_gd.shape)\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history_gd)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Batch Gradient Descent Convergence\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8d79f",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "In Stochastic Gradient Descent (SGD), we update the parameters using one training example\n",
    "at a time.  This can converge faster and allows online training on streaming data.  The\n",
    "update rule for a single sample \\(i\\) is:\n",
    "\n",
    "\\[\n",
    "\\theta := \\theta - 2 \\alpha \\bigl( h_\\theta(x^{(i)}) - y^{(i)} \\bigr) \\tilde{x}^{(i)}.\n",
    "\\]\n",
    "\n",
    "We shuffle the training data each epoch to ensure the gradient direction varies over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(Xb: np.ndarray, y: np.ndarray, lr: float = 0.01, epochs: int = 50, seed: int = 0) -> tuple[np.ndarray, list]:\n",
    "    \"\"\"Perform stochastic gradient descent.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    m = Xb.shape[0]\n",
    "    theta = np.zeros((Xb.shape[1], 1))\n",
    "    loss_history = []\n",
    "    for epoch in range(epochs):\n",
    "        indices = rng.permutation(m)\n",
    "        for i in indices:\n",
    "            xi = Xb[i:i+1]  # shape (1, n+1)\n",
    "            yi = y[i:i+1]   # shape (1, 1)\n",
    "            error = xi @ theta - yi\n",
    "            theta -= lr * 2 * error * xi.T\n",
    "        loss_history.append(mse_loss(Xb, y, theta))\n",
    "    return theta, loss_history\n",
    "\n",
    "theta_sgd, history_sgd = sgd(Xb, y, lr=0.01, epochs=100)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history_sgd)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Stochastic Gradient Descent Convergence\")\n",
    "plt.show()\n",
    "\n",
    "print(\"SGD final loss:\", history_sgd[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd580f5e",
   "metadata": {},
   "source": [
    "### Visualization of Individual Features\n",
    "\n",
    "For interpretability we plot the learned regression line against the true data for a few\n",
    "selected features.  To keep plots readable we display the first four features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [f\"Feature {i}\" for i in range(n)]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for idx in range(4):\n",
    "    Xi = X[:, idx].reshape(-1, 1)\n",
    "    # Build line for plotting\n",
    "    x_line = np.linspace(Xi.min(), Xi.max(), 200).reshape(-1, 1)\n",
    "    Xb_line = np.zeros((len(x_line), Xb.shape[1]))\n",
    "    Xb_line[:, 0] = 1\n",
    "    Xb_line[:, idx + 1] = x_line[:, 0]\n",
    "    y_line = Xb_line @ theta_gd\n",
    "\n",
    "    plt.subplot(2, 2, idx + 1)\n",
    "    plt.scatter(Xi, y, alpha=0.2)\n",
    "    plt.plot(x_line, y_line, color=\"red\")\n",
    "    plt.xlabel(feature_names[idx])\n",
    "    plt.ylabel(\"Target\")\n",
    "    plt.title(f\"Linear Fit for {feature_names[idx]}\")\n",
    "plt.suptitle(\"Model Fits for Selected Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb2f5d",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Normal equation derivation**: Show that the optimal \\(\\theta\\) minimizing the MSE has a closed form\n",
    "   \\(\\theta = (X_b^T X_b)^{-1} X_b^T y\\).  Implement this formula and compare it to the gradient descent\n",
    "   result.\n",
    "2. **Learning rate tuning**: Experiment with different learning rates and epochs for batch gradient descent.\n",
    "   Plot the learning curves to see how they influence convergence.\n",
    "3. **Feature selection**: Use only a subset of features (e.g. top 2 principal components) and retrain the model.\n",
    "   How does feature dimensionality affect performance?\n",
    "4. **Regularization**: Add an \\(\\ell_2\\) regularization term \\(\\lambda \\lVert \\theta \\rVert^2\\) to the cost function\n",
    "   and derive the new gradient update.  Implement gradient descent with regularization.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- The linear regression model assumes a linear relationship between features and the response:\n",
    "  \\(h_\\theta(x) = \\theta^T \\tilde{x}\\).\n",
    "- The mean squared error cost is convex; its gradient is \\(\\nabla J(\\theta) = \\frac{2}{m} X_b^T (X_b \\theta - y)\\).\n",
    "- **Gradient descent** updates parameters by taking steps opposite the gradient: \\(\\theta := \\theta - \\alpha \\nabla J(\\theta)\\).\n",
    "- **Batch vs. Stochastic**: batch uses all data per update; stochastic uses one sample.  SGD often converges\n",
    "  faster but has higher variance in updates.\n",
    "- Adding an intercept term allows the model to fit a bias.  Standardizing features improves conditioning of the\n",
    "  problem.\n",
    "- Closed‑form solution exists via the normal equation, but iterative methods scale better to large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
