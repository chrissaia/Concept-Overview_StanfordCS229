{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00425474",
   "metadata": {},
   "source": [
    "# Lesson 19 — Linear Quadratic Regulator (LQR)\n",
    "\n",
    "The Linear Quadratic Regulator (LQR) solves continuous state‐space control problems\n",
    "with linear dynamics and quadratic cost.  It computes an optimal feedback gain\n",
    "matrix that minimizes the expected infinite‑horizon cost.  In this notebook we\n",
    "derive and implement the discrete‑time LQR solution, simulate the controlled\n",
    "system and analyze the resulting trajectories.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Problem formulation**: linear dynamics, quadratic cost.\n",
    "- **Riccati equation**: derive the discrete Riccati difference equation.\n",
    "- **Optimal feedback gain**: compute steady‑state solution.\n",
    "- **Simulation**: apply control law to the system and plot trajectories.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9f655",
   "metadata": {},
   "source": [
    "### Imports & System Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define a simple 1D linear system: x_{t+1} = a x_t + b u_t\n",
    "a = 1.1\n",
    "b = 0.5\n",
    "A = np.array([[a]])\n",
    "B = np.array([[b]])\n",
    "\n",
    "# Cost function: x^T Q x + u^T R u\n",
    "Q = np.array([[1.0]])\n",
    "R = np.array([[0.1]])\n",
    "\n",
    "# Discount factor (for infinite horizon) – set to 1 for undiscounted LQR\n",
    "gamma = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73d924",
   "metadata": {},
   "source": [
    "### Riccati Equation & Optimal Gain\n",
    "\n",
    "The optimal feedback control law for the discrete‑time LQR is \\(u = -K x\\), where\n",
    "\\(K\\) is computed from the solution \\(P\\) of the algebraic Riccati equation:\n",
    "\n",
    "\\[\n",
    "P = Q + A^T P A - A^T P B (R + B^T P B)^{-1} B^T P A.\n",
    "\\]\n",
    "\n",
    "We iterate the Riccati difference equation until convergence to obtain \\(P\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1995ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_discrete_lqr(A: np.ndarray, B: np.ndarray, Q: np.ndarray, R: np.ndarray, gamma: float = 1.0, tol: float = 1e-8, max_iters: int = 1000):\n",
    "    \"\"\"Solve the discrete Riccati equation for LQR.\"\"\"\n",
    "    P = Q.copy()\n",
    "    for _ in range(max_iters):\n",
    "        K = np.linalg.inv(R + gamma * B.T @ P @ B) @ (gamma * B.T @ P @ A)\n",
    "        P_next = Q + gamma * A.T @ P @ A - gamma * A.T @ P @ B @ K\n",
    "        if np.max(np.abs(P_next - P)) < tol:\n",
    "            P = P_next\n",
    "            break\n",
    "        P = P_next\n",
    "    K_opt = np.linalg.inv(R + gamma * B.T @ P @ B) @ (gamma * B.T @ P @ A)\n",
    "    return P, K_opt\n",
    "\n",
    "\n",
    "# Solve for P and K\n",
    "P, K = solve_discrete_lqr(A, B, Q, R, gamma)\n",
    "print(\"Optimal gain K:\", K.flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8258241",
   "metadata": {},
   "source": [
    "### Simulating the Closed‑Loop System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_lqr(A, B, K, x0, T=30):\n",
    "    \"\"\"Simulate the closed‑loop linear system with feedback u = -Kx.\"\"\"\n",
    "    x = np.zeros((T + 1, A.shape[0]))\n",
    "    u = np.zeros((T, B.shape[1]))\n",
    "    x[0] = x0\n",
    "    for t in range(T):\n",
    "        u[t] = -K @ x[t]\n",
    "        x[t + 1] = A @ x[t] + B @ u[t]\n",
    "    return x, u\n",
    "\n",
    "# Initial state\n",
    "x0 = np.array([5.0])\n",
    "\n",
    "# Simulate\n",
    "T = 20\n",
    "x_traj, u_traj = simulate_lqr(A, B, K, x0, T)\n",
    "\n",
    "# Plot state and control trajectories\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_traj)\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"State x\")\n",
    "plt.title(\"State Trajectory under LQR Control\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(u_traj)\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Control u\")\n",
    "plt.title(\"Control Trajectory\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b670ad4",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Multi‑Dimensional Systems**: Extend the example to higher‑dimensional systems (e.g.,\n",
    "   cart–pole linearization).  Derive the corresponding A and B matrices and solve\n",
    "   for the optimal feedback gain.\n",
    "2. **Discounting**: Introduce \\(\\gamma < 1\\) into the Riccati iteration and observe how the\n",
    "   gain and state trajectories change.\n",
    "3. **Finite Horizon**: For a finite horizon T, implement the backward dynamic\n",
    "   programming recursion to compute time‑varying gains \\(K_t\\).\n",
    "4. **Continuous‑Time LQR**: Derive and implement the continuous‑time LQR solution\n",
    "   using the continuous Riccati differential equation.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- LQR addresses control of linear dynamical systems with quadratic costs, yielding\n",
    "  closed‑form optimal feedback gains computed via the solution of a Riccati equation.\n",
    "- The discrete algebraic Riccati equation can be solved iteratively; the resulting\n",
    "  gain matrix produces a stabilizing controller that minimizes the infinite‑horizon cost.\n",
    "- The cost matrices Q and R weight state deviations and control effort, respectively.\n",
    "  Tuning Q and R trades off between accuracy (tracking) and energy usage.\n",
    "- LQR extends naturally to multi‑dimensional systems, continuous time and finite\n",
    "  horizons, and forms the basis of more advanced control methods such as LQG\n",
    "  (Linear Quadratic Gaussian) and MPC (Model Predictive Control)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
