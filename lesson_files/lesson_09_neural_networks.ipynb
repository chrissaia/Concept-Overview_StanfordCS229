{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46b00b8",
   "metadata": {},
   "source": [
    "# Lesson 9 — Neural Networks & Backpropagation\n",
    "\n",
    "Neural networks are flexible function approximators composed of layers of linear\n",
    "transformations followed by nonlinear activation functions.  In this notebook we\n",
    "build a simple feedforward neural network from scratch using **NumPy** to classify\n",
    "handwritten digits.  We implement forward propagation, backpropagation and gradient\n",
    "descent to train the network on the `digits` dataset from scikit‑learn.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Data loading & preprocessing**: load digit images, flatten and normalize.\n",
    "- **Network architecture**: one hidden layer with nonlinear activation.\n",
    "- **Forward pass**: compute activations for hidden and output layers.\n",
    "- **Backpropagation**: derive gradients of weights and biases.\n",
    "- **Training loop**: update parameters via gradient descent.\n",
    "- **Evaluation & visualization**: report accuracy and show some predictions.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bdc0b8",
   "metadata": {},
   "source": [
    "### Imports & Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # datasets only\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the handwritten digits dataset (64 features, 10 classes)\n",
    "digits = datasets.load_digits()\n",
    "X_raw = digits.data  # shape (1797, 64)\n",
    "y_raw = digits.target.reshape(-1, 1)  # shape (1797, 1)\n",
    "\n",
    "# Normalize features to [0, 1]\n",
    "X = X_raw / 16.0\n",
    "\n",
    "num_classes = int(np.max(y_raw)) + 1\n",
    "y_onehot = np.eye(num_classes)[y_raw.flatten()]\n",
    "\n",
    "# Split into training and test sets\n",
    "rng = np.random.default_rng(42)\n",
    "indices = rng.permutation(X.shape[0])\n",
    "split = int(0.8 * X.shape[0])\n",
    "train_idx = indices[:split]\n",
    "test_idx = indices[split:]\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y_onehot[train_idx], y_onehot[test_idx]\n",
    "\n",
    "print(f\"Digits dataset: {X_train.shape[0]} training examples, {X_test.shape[0]} test examples.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed9942",
   "metadata": {},
   "source": [
    "### Network Architecture & Initialization\n",
    "\n",
    "We define a neural network with one hidden layer.  Let the input dimension be\n",
    "\\(n_0 = 64\\), hidden layer size \\(n_1\\), and output dimension \\(n_2 = 10\\).  The parameters\n",
    "consist of weight matrices \\(W^{(1)} \\in \\mathbb{R}^{n_0 \\times n_1}\\) and \\(W^{(2)} \\in \\mathbb{R}^{n_1 \\times n_2}\\),\n",
    "and bias vectors \\(b^{(1)} \\in \\mathbb{R}^{n_1}\\) and \\(b^{(2)} \\in \\mathbb{R}^{n_2}\\).  We use the hyperbolic\n",
    "tangent activation for the hidden layer and the softmax function for the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 32\n",
    "output_dim = y_train.shape[1]\n",
    "\n",
    "# Initialize weights with small random values and biases with zeros\n",
    "W1 = 0.01 * np.random.randn(input_dim, hidden_dim)\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = 0.01 * np.random.randn(hidden_dim, output_dim)\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213766a5",
   "metadata": {},
   "source": [
    "### Activation Functions and Forward Pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2123eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute softmax row‑wise in a numerically stable way.\"\"\"\n",
    "    z_shift = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z_shift)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def forward(X: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute activations for the hidden and output layers.\"\"\"\n",
    "    Z1 = X @ W1 + b1  # hidden pre‑activation\n",
    "    A1 = np.tanh(Z1)  # hidden activation\n",
    "    Z2 = A1 @ W2 + b2  # output pre‑activation\n",
    "    A2 = softmax(Z2)   # output probabilities\n",
    "    return Z1, A1, Z2, A2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717a15e",
   "metadata": {},
   "source": [
    "### Loss Function and Backpropagation\n",
    "\n",
    "We use the multiclass cross‑entropy loss:\n",
    "\n",
    "\\[\n",
    "J = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^{n_2} y^{(i)}_k \\log \\hat{y}^{(i)}_k,\n",
    "\\]\n",
    "\n",
    "where \\(\\hat{y}^{(i)}\\) are the softmax outputs.  The gradients can be derived using\n",
    "the chain rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a3850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    m = y_true.shape[0]\n",
    "    # add small epsilon to avoid log(0)\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
    "    return float(loss)\n",
    "\n",
    "def backward(X: np.ndarray, y_true: np.ndarray, Z1: np.ndarray, A1: np.ndarray, A2: np.ndarray, W2: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute gradients of weights and biases via backpropagation.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    # Gradient of loss w.r.t. output pre‑activation\n",
    "    dZ2 = (A2 - y_true) / m  # shape (m, n2)\n",
    "    dW2 = A1.T @ dZ2         # shape (n1, n2)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    # Gradient w.r.t. hidden layer\n",
    "    dA1 = dZ2 @ W2.T             # shape (m, n1)\n",
    "    dZ1 = dA1 * (1 - np.tanh(Z1) ** 2)  # derivative of tanh\n",
    "    dW1 = X.T @ dZ1              # shape (n0, n1)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037897c2",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Mini‑batch gradient descent\n",
    "    perm = np.random.permutation(X_train.shape[0])\n",
    "    X_shuffled = X_train[perm]\n",
    "    y_shuffled = y_train[perm]\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "        # Forward\n",
    "        Z1, A1, Z2, A2 = forward(X_batch, W1, b1, W2, b2)\n",
    "        # Backward\n",
    "        dW1, db1, dW2, db2 = backward(X_batch, y_batch, Z1, A1, A2, W2)\n",
    "        # Update\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "    # Compute loss on full training set for monitoring\n",
    "    _, _, _, A2_full = forward(X_train, W1, b1, W2, b2)\n",
    "    loss = compute_loss(A2_full, y_train)\n",
    "    loss_history.append(loss)\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross‑Entropy Loss\")\n",
    "plt.title(\"Neural Network Training Loss\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d348c",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy & Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8361a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(X: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Predict class labels for a dataset.\"\"\"\n",
    "    _, _, _, A2 = forward(X, W1, b1, W2, b2)\n",
    "    return np.argmax(A2, axis=1).reshape(-1, 1)\n",
    "\n",
    "# Training accuracy\n",
    "y_train_pred = predict_nn(X_train, W1, b1, W2, b2)\n",
    "y_train_true = np.argmax(y_train, axis=1).reshape(-1, 1)\n",
    "train_accuracy = (y_train_pred == y_train_true).mean()\n",
    "\n",
    "# Test accuracy\n",
    "y_test_pred = predict_nn(X_test, W1, b1, W2, b2)\n",
    "y_test_true = np.argmax(y_test, axis=1).reshape(-1, 1)\n",
    "test_accuracy = (y_test_pred == y_test_true).mean()\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Show a few test predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    idx = np.random.randint(0, X_test.shape[0])\n",
    "    ax.imshow(digits.images[digits.target == y_test_true[idx]][0], cmap='gray')  # show any sample of that digit\n",
    "    ax.set_title(f\"Pred: {int(y_test_pred[idx])}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Sample Digit Predictions (labels may not match samples)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14d369",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Hidden Layer Size**: Experiment with different numbers of hidden units and observe\n",
    "   how training and test accuracy change.  Does a larger network always perform better?\n",
    "2. **Activation Functions**: Replace `tanh` with ReLU (rectified linear unit) or sigmoid and\n",
    "   modify the backpropagation accordingly.  Compare convergence and performance.\n",
    "3. **Momentum & Regularization**: Implement momentum or Adam optimizer to accelerate\n",
    "   training.  Add \\(\\ell_2\\) regularization to the weight gradients and observe its effect.\n",
    "4. **Deep Networks**: Add another hidden layer and derive the backpropagation equations.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- A feedforward neural network is composed of layers of linear transforms followed by\n",
    "  nonlinear activation functions.  The weights and biases are trained to minimize a\n",
    "  loss function via gradient descent.\n",
    "- Backpropagation efficiently computes gradients by applying the chain rule from the\n",
    "  output layer backwards to the input.  Each layer's gradient depends on the\n",
    "  derivative of its activation function and the gradients of subsequent layers.\n",
    "- The softmax function converts raw scores into probabilities that sum to one.  It is\n",
    "  paired with the cross‑entropy loss for multiclass classification.\n",
    "- Proper initialization and normalization (e.g., dividing pixel values) help\n",
    "  accelerate convergence.  Choice of activation and network size affects capacity and\n",
    "  risk of overfitting.\n",
    "- More sophisticated optimizers (e.g., Adam) and regularization techniques can\n",
    "  significantly improve training speed and generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
