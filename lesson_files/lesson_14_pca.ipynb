{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3aeb6c",
   "metadata": {},
   "source": [
    "# Lesson 14 — Dimensionality Reduction: Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is a widely used technique for reducing the\n",
    "dimensionality of a dataset while preserving as much variance as possible.  It\n",
    "projects data onto a lower‑dimensional subspace spanned by the leading eigenvectors\n",
    "of the covariance matrix.  In this notebook we implement PCA using the singular\n",
    "value decomposition (SVD) and apply it to the digits dataset.  We also explore\n",
    "reconstruction of compressed images.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Data loading & centering**: load digit images and subtract the mean.\n",
    "- **SVD & principal components**: compute components and explained variance.\n",
    "- **Projection & reconstruction**: compress data and reconstruct images.\n",
    "- **Visualization**: display original and reconstructed digits and plot variance ratios.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dadbf8",
   "metadata": {},
   "source": [
    "### Imports & Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # datasets only\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load digits dataset (64 features, 8x8 images)\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data  # shape (1797, 64)\n",
    "images = digits.images\n",
    "\n",
    "# Center the data (subtract mean)\n",
    "X_mean = X.mean(axis=0)\n",
    "X_centered = X - X_mean\n",
    "\n",
    "print(f\"Centered digits dataset: {X_centered.shape[0]} samples, {X_centered.shape[1]} features.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc67be3f",
   "metadata": {},
   "source": [
    "### PCA via Singular Value Decomposition\n",
    "\n",
    "Compute SVD of centered data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "\n",
    "# Explained variance of each component: (S^2)/(n_samples - 1)\n",
    "explained_variances = (S ** 2) / (X_centered.shape[0] - 1)\n",
    "explained_variance_ratio = explained_variances / explained_variances.sum()\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(np.arange(1, len(cumulative_variance) + 1), cumulative_variance)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance on Digits\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d504dcd",
   "metadata": {},
   "source": [
    "### Projection & Reconstruction\n",
    "\n",
    "We choose a small number of principal components (e.g. 16) and project the data\n",
    "onto this subspace.  We then reconstruct the images by projecting back to the\n",
    "original space and adding the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 16\n",
    "V_top = Vt[:n_components]  # shape (n_components, 64)\n",
    "\n",
    "# Project data to lower dimension\n",
    "X_projected = X_centered @ V_top.T\n",
    "# Reconstruct from projection\n",
    "X_reconstructed = X_projected @ V_top + X_mean\n",
    "\n",
    "print(f\"Data compressed from 64 to {n_components} dimensions.\")\n",
    "\n",
    "# Visualize original and reconstructed images for a few samples\n",
    "num_samples = 5\n",
    "indices = np.random.choice(X.shape[0], num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i, idx in enumerate(indices):\n",
    "    # Original\n",
    "    ax = plt.subplot(2, num_samples, i + 1)\n",
    "    ax.imshow(images[idx], cmap='gray')\n",
    "    ax.axis('off')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Original\")\n",
    "    # Reconstructed\n",
    "    ax = plt.subplot(2, num_samples, i + 1 + num_samples)\n",
    "    ax.imshow(X_reconstructed[idx].reshape(8, 8), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Reconstructed\")\n",
    "plt.suptitle(f\"PCA Reconstruction with {n_components} Components\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775adba",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Choosing the Number of Components**: Find the minimal number of components that\n",
    "   explain at least 95% of the variance.  Visualize the cumulative variance curve\n",
    "   and justify your choice.\n",
    "2. **Noise Reduction**: Add Gaussian noise to the digit images and perform PCA\n",
    "   reconstruction.  Observe how PCA can act as a denoising method by discarding\n",
    "   components corresponding to noise.\n",
    "3. **Independent Component Analysis (ICA)**: Implement or use a library (e.g.,\n",
    "   scikit‑learn's `FastICA`) to extract independent components.  Compare the\n",
    "   components to the principal components.\n",
    "4. **Applications**: Apply PCA to other datasets (e.g., face images, gene expression\n",
    "   data) and discuss its utility and limitations.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- **PCA** projects data onto orthogonal directions (principal components) that maximize\n",
    "  variance.  It can be computed via eigenvalue decomposition of the covariance\n",
    "  matrix or via singular value decomposition of the centered data matrix.\n",
    "- The explained variance ratio indicates how much information each principal\n",
    "  component retains; cumulative plots help decide how many components to keep.\n",
    "- PCA is commonly used for dimensionality reduction, visualization and noise\n",
    "  reduction.  Reconstruction error decreases as more components are retained.\n",
    "- Unlike PCA, **Independent Component Analysis (ICA)** seeks statistically independent\n",
    "  sources rather than uncorrelated directions.  ICA is useful for separating mixed\n",
    "  signals (e.g., cocktail party problem)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
