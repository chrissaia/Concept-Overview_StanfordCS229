{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3f24f5",
   "metadata": {},
   "source": [
    "# Lesson 3 — Logistic Regression from Scratch\n",
    "\n",
    "In this notebook we derive and implement logistic regression using only **NumPy**.  We\n",
    "discuss the underlying probabilistic interpretation, derive the gradient of the\n",
    "cross‑entropy loss, and implement gradient descent to fit the model.  We'll work on\n",
    "a binary classification dataset to illustrate the decision boundary and measure\n",
    "performance.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Problem setup & notation**: binary classification, logistic hypothesis and cost.\n",
    "- **Data loading & preprocessing**: load a real dataset and standardize features.\n",
    "- **Gradient descent implementation**: derive and code the update rule.\n",
    "- **Model evaluation**: compute accuracy and visualize the decision boundary.\n",
    "- **Exercises & interview summary**: practice questions and key takeaways.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7951e0",
   "metadata": {},
   "source": [
    "### Problem Setup & Notation\n",
    "\n",
    "Consider a dataset \\(\\{(x^{(i)}, y^{(i)})\\}\\_{i=1}^m\\) where \\(y^{(i)} \\in \\{0,1\\}\\) indicates class\n",
    "membership.  We augment each feature vector with a 1 to obtain \\(\\tilde{x}^{(i)} \\in \\mathbb{R}^{n+1}\\).\n",
    "The logistic regression hypothesis uses the sigmoid function:\n",
    "\n",
    "\\[\n",
    "h_\\theta(x) = \\sigma(\\theta^T \\tilde{x}) = \\frac{1}{1 + e^{-\\theta^T \\tilde{x}}}.\n",
    "\\]\n",
    "\n",
    "The cost function is the negative log‑likelihood (cross‑entropy):\n",
    "\n",
    "\\[\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)}) \\log (1 - h_\\theta(x^{(i)}))\\right].\n",
    "\\]\n",
    "\n",
    "Differentiating \\(J\\) with respect to \\(\\theta\\) yields the gradient\n",
    "\n",
    "\\[\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{m} X_b^T \\bigl(h_\\theta(X) - y\\bigr),\n",
    "\\]\n",
    "where \\(X_b\\) is the design matrix with an intercept column.  We will implement gradient\n",
    "descent to minimize this cost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2725f",
   "metadata": {},
   "source": [
    "### Imports & Random Seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383534c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # datasets only\n",
    "\n",
    "np.random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b3d03a",
   "metadata": {},
   "source": [
    "### Data Loading & Preprocessing\n",
    "\n",
    "We use the **breast cancer** dataset from `sklearn.datasets`, a binary classification problem with\n",
    "30 features.  We standardize the features to have zero mean and unit variance and map\n",
    "the labels to 0 and 1.  Then we add an intercept term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3500b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, y_raw = datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Standardize features\n",
    "X = (X_raw - X_raw.mean(axis=0)) / X_raw.std(axis=0)\n",
    "y = y_raw.reshape(-1, 1)\n",
    "\n",
    "# Number of examples and features\n",
    "m, n = X.shape\n",
    "\n",
    "# Add intercept column\n",
    "Xb = np.hstack([np.ones((m, 1)), X])  # shape (m, n+1)\n",
    "\n",
    "print(f\"Dataset: {m} examples, {n} features (after standardization).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80600cb",
   "metadata": {},
   "source": [
    "### Logistic Hypothesis, Loss and Gradient Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed669e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute the sigmoid function elementwise.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def log_loss(Xb: np.ndarray, y: np.ndarray, theta: np.ndarray) -> float:\n",
    "    \"\"\"Compute the cross‑entropy loss for logistic regression.\"\"\"\n",
    "    m = Xb.shape[0]\n",
    "    h = sigmoid(Xb @ theta)\n",
    "    # Clip h to avoid log(0)\n",
    "    h = np.clip(h, 1e-15, 1 - 1e-15)\n",
    "    loss = - (y * np.log(h) + (1 - y) * np.log(1 - h)).mean()\n",
    "    return float(loss)\n",
    "\n",
    "def log_gradient(Xb: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute the gradient of the logistic loss.\"\"\"\n",
    "    m = Xb.shape[0]\n",
    "    h = sigmoid(Xb @ theta)\n",
    "    return (1.0 / m) * Xb.T @ (h - y)\n",
    "\n",
    "def gradient_descent_logistic(Xb: np.ndarray, y: np.ndarray, lr: float = 0.1, epochs: int = 2000) -> tuple[np.ndarray, list]:\n",
    "    \"\"\"Perform gradient descent for logistic regression.\"\"\"\n",
    "    theta = np.zeros((Xb.shape[1], 1))\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = log_loss(Xb, y, theta)\n",
    "        history.append(loss)\n",
    "        grad = log_gradient(Xb, y, theta)\n",
    "        theta -= lr * grad\n",
    "    return theta, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eda7c9",
   "metadata": {},
   "source": [
    "### Training the Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "\n",
    "theta_lr, history_lr = gradient_descent_logistic(Xb, y, lr=learning_rate, epochs=epochs)\n",
    "\n",
    "print(\"Final loss:\", history_lr[-1])\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history_lr)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross‑Entropy Loss\")\n",
    "plt.title(\"Logistic Regression Convergence\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2056848",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "To evaluate the classifier we predict labels using a 0.5 threshold on \\(h_\\theta(x)\\).\n",
    "We compute the accuracy and visualize the decision boundary using the first two features for\n",
    "illustration.  Note that the true classification uses all features, but we can project\n",
    "onto two dimensions for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654faadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Xb: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Predict binary labels using the learned parameters.\"\"\"\n",
    "    probs = sigmoid(Xb @ theta)\n",
    "    return (probs >= 0.5).astype(int)\n",
    "\n",
    "preds = predict(Xb, theta_lr)\n",
    "accuracy = (preds == y).mean()\n",
    "print(f\"Training accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Visualization using two features (indices 0 and 1)\n",
    "feat1, feat2 = 0, 1\n",
    "X2 = X[:, [feat1, feat2]]\n",
    "X2b = np.hstack([np.ones((m, 1)), X2])\n",
    "\n",
    "# Compute line parameters for decision boundary using only two features\n",
    "theta_2d = theta_lr[[0, feat1 + 1, feat2 + 1]]  # intercept + selected weights\n",
    "# Decision boundary: theta_0 + theta_1 x1 + theta_2 x2 = 0 => x2 = -(theta_0 + theta_1 x1)/theta_2\n",
    "x_vals = np.linspace(X2[:, 0].min(), X2[:, 0].max(), 200)\n",
    "if abs(theta_2d[2, 0]) > 1e-6:\n",
    "    y_vals = - (theta_2d[0, 0] + theta_2d[1, 0] * x_vals) / theta_2d[2, 0]\n",
    "else:\n",
    "    y_vals = np.zeros_like(x_vals)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X2[y[:, 0] == 0][:, 0], X2[y[:, 0] == 0][:, 1], alpha=0.5, label=\"Class 0\")\n",
    "plt.scatter(X2[y[:, 0] == 1][:, 0], X2[y[:, 0] == 1][:, 1], alpha=0.5, label=\"Class 1\")\n",
    "plt.plot(x_vals, y_vals, 'r-', label=\"Decision boundary (2D projection)\")\n",
    "plt.xlabel(f\"Feature {feat1}\")\n",
    "plt.ylabel(f\"Feature {feat2}\")\n",
    "plt.title(\"Logistic Regression Decision Boundary (First Two Features)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee715c17",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Newton's Method**: Derive the Hessian for logistic regression and implement Newton's\n",
    "   method for optimization.  Compare its convergence to gradient descent.\n",
    "2. **Multiclass Logistic Regression**: Extend the model to handle multiple classes via\n",
    "   softmax and cross‑entropy loss.  Implement gradient descent for the multiclass case.\n",
    "3. **Regularization**: Add \\(\\ell_2\\) or \\(\\ell_1\\) regularization to the cost function and study its\n",
    "   effect on the weight vector and decision boundary.\n",
    "4. **Feature Engineering**: Explore adding polynomial terms or interactions between\n",
    "   features.  How does this change the classification accuracy?\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- Logistic regression models the conditional probability \\(P(y=1 \\mid x)\\) using the\n",
    "  sigmoid of a linear combination of features.\n",
    "- The cross‑entropy loss is convex; its gradient is the difference between predicted\n",
    "  probabilities and labels scaled by the features: \\(\\nabla J = \\frac{1}{m} X_b^T(h - y)\\).\n",
    "- Gradient descent iteratively updates parameters by subtracting the gradient scaled by\n",
    "  the learning rate.  A good choice of learning rate and number of epochs is critical\n",
    "  for convergence.\n",
    "- Thresholding the output at 0.5 yields binary predictions; accuracy is a common metric\n",
    "  but can be complemented by precision, recall and F1 score for imbalanced data.\n",
    "- Logistic regression can be regularized to prevent overfitting and extended to\n",
    "  multiclass problems via the softmax function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
