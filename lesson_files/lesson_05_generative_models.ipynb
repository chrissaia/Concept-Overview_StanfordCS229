{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864c55f5",
   "metadata": {},
   "source": [
    "# Lesson 5 — Generative Learning: GDA & Naive Bayes\n",
    "\n",
    "In this notebook we explore **generative models** for classification.  Instead of\n",
    "modeling \\(P(y \\mid x)\\) directly as in discriminative methods like logistic regression,\n",
    "generative algorithms model the joint distribution \\(P(x, y)\\) or \\(P(x \\mid y) P(y)\\) and then\n",
    "apply Bayes' rule.  We focus on two classical algorithms:\n",
    "\n",
    "1. **Gaussian Discriminant Analysis (GDA)**: assumes the class‑conditional distribution\n",
    "   \\(x \\mid y\\) is multivariate normal with class‑specific means and a shared covariance\n",
    "   matrix.\n",
    "2. **Naive Bayes**: assumes features are conditionally independent given the class.\n",
    "   We implement a Gaussian naive Bayes classifier where each feature is modeled as\n",
    "   univariate normal.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Data loading & preprocessing**: binary classification dataset.\n",
    "- **GDA derivation & implementation**: estimate parameters and predict.\n",
    "- **Gaussian Naive Bayes**: estimate means/variances for each feature per class.\n",
    "- **Comparison & evaluation**: accuracy and decision boundaries.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4aecf8",
   "metadata": {},
   "source": [
    "### Imports & Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # datasets only\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load breast cancer dataset\n",
    "X_raw, y_raw = datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Standardize features\n",
    "X = (X_raw - X_raw.mean(axis=0)) / X_raw.std(axis=0)\n",
    "y = y_raw.reshape(-1, 1)\n",
    "\n",
    "# Split into training and testing sets (80/20 split)\n",
    "def train_test_split(X: np.ndarray, y: np.ndarray, test_ratio: float = 0.2, seed: int = 0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    m = X.shape[0]\n",
    "    indices = rng.permutation(m)\n",
    "    test_size = int(m * test_ratio)\n",
    "    test_idx = indices[:test_size]\n",
    "    train_idx = indices[test_size:]\n",
    "    return X[train_idx], y[train_idx], X[test_idx], y[test_idx]\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "m_train, n = X_train.shape\n",
    "\n",
    "print(f\"Training set: {m_train} examples, Testing set: {X_test.shape[0]} examples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f45f98",
   "metadata": {},
   "source": [
    "### Gaussian Discriminant Analysis (GDA)\n",
    "\n",
    "In GDA we assume that for each class \\(c \\in \\{0,1\\}\\), the conditional distribution\n",
    "\\(x \\mid y=c\\) is multivariate normal with mean \\(\\mu_c\\) and shared covariance matrix\n",
    "\\(\\Sigma\\).  The class priors are \\(\\phi_c = P(y=c)\\).  The parameters are estimated as:\n",
    "\n",
    "\\[\n",
    "\\phi_c = \\frac{1}{m} \\sum_{i=1}^m 1\\{y^{(i)} = c\\},\\quad\n",
    "\\mu_c = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = c\\} x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = c\\}},\\quad\n",
    "\\Sigma = \\frac{1}{m} \\sum_{i=1}^m \\bigl(x^{(i)} - \\mu_{y^{(i)}}\\bigr) \\bigl(x^{(i)} - \\mu_{y^{(i)}}\\bigr)^T.\n",
    "\\]\n",
    "\n",
    "The log posterior difference for class 1 vs class 0 can be written as a linear function of\n",
    "\\(x\\).  We derive a weight vector and bias term that yields a logistic form similar to\n",
    "logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f21683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gda_fit(X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Estimate parameters for Gaussian Discriminant Analysis.\n",
    "\n",
    "    Returns:\n",
    "        phi (np.ndarray): class prior for y=1\n",
    "        mu_0, mu_1 (np.ndarray): class means (shape (n, 1))\n",
    "        Sigma (np.ndarray): shared covariance matrix (shape (n, n))\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    y = y.flatten()\n",
    "    phi = y.mean()\n",
    "    mu_0 = X[y == 0].mean(axis=0).reshape(-1, 1)\n",
    "    mu_1 = X[y == 1].mean(axis=0).reshape(-1, 1)\n",
    "    Sigma = np.zeros((n, n))\n",
    "    for i in range(m):\n",
    "        xi = X[i].reshape(-1, 1)\n",
    "        mu = mu_1 if y[i] == 1 else mu_0\n",
    "        Sigma += (xi - mu) @ (xi - mu).T\n",
    "    Sigma /= m\n",
    "    return phi, mu_0, mu_1, Sigma\n",
    "\n",
    "def gda_predict(X: np.ndarray, phi: float, mu_0: np.ndarray, mu_1: np.ndarray, Sigma: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Predict class labels using GDA parameters.\"\"\"\n",
    "    # Compute inverse and determinant once\n",
    "    Sigma_inv = np.linalg.pinv(Sigma)\n",
    "    # Discriminant scores for class 1 and 0\n",
    "    def log_gaussian(x: np.ndarray, mu: np.ndarray) -> float:\n",
    "        diff = x - mu\n",
    "        return -0.5 * float(diff.T @ Sigma_inv @ diff)\n",
    "    preds = []\n",
    "    for xi in X:\n",
    "        xi_col = xi.reshape(-1, 1)\n",
    "        log_p1 = log_gaussian(xi_col, mu_1) + np.log(phi + 1e-15)\n",
    "        log_p0 = log_gaussian(xi_col, mu_0) + np.log(1 - phi + 1e-15)\n",
    "        preds.append(1 if log_p1 > log_p0 else 0)\n",
    "    return np.array(preds).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Fit GDA parameters\n",
    "phi, mu_0, mu_1, Sigma = gda_fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set and compute accuracy\n",
    "gda_preds = gda_predict(X_test, phi, mu_0, mu_1, Sigma)\n",
    "gda_accuracy = (gda_preds == y_test).mean()\n",
    "print(f\"GDA test accuracy: {gda_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8944569",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "Under the naive Bayes assumption, features are conditionally independent given the class,\n",
    "so the class‑conditional density factors as a product of univariate normals.  For each\n",
    "class \\(c\\) and feature \\(j\\), we estimate the mean \\(\\mu_{c,j}\\) and variance \\(\\sigma^2_{c,j}\\) from\n",
    "the training data.  The posterior can then be computed via\n",
    "\n",
    "\\[\n",
    "P(y=1 \\mid x) \\propto \\phi \\prod_{j=1}^n \\mathcal{N}(x_j; \\mu_{1,j}, \\sigma^2_{1,j}),\n",
    "\\]\n",
    "and similarly for \\(y=0\\).  Taking logs avoids underflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f93d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_fit(X: np.ndarray, y: np.ndarray) -> tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Estimate parameters for Gaussian Naive Bayes.\n",
    "\n",
    "    Returns:\n",
    "        phi: class prior for y=1\n",
    "        mu_0, mu_1: mean vectors for each class (shape (n, 1))\n",
    "        var_0, var_1: variance vectors for each class (shape (n, 1))\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    y = y.flatten()\n",
    "    phi = y.mean()\n",
    "    mu_0 = X[y == 0].mean(axis=0).reshape(-1, 1)\n",
    "    mu_1 = X[y == 1].mean(axis=0).reshape(-1, 1)\n",
    "    var_0 = X[y == 0].var(axis=0).reshape(-1, 1) + 1e-9  # add epsilon to avoid zeros\n",
    "    var_1 = X[y == 1].var(axis=0).reshape(-1, 1) + 1e-9\n",
    "    return phi, mu_0, mu_1, var_0, var_1\n",
    "\n",
    "def naive_bayes_predict(X: np.ndarray, phi: float, mu_0: np.ndarray, mu_1: np.ndarray, var_0: np.ndarray, var_1: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Predict class labels using Gaussian Naive Bayes parameters.\"\"\"\n",
    "    n = X.shape[1]\n",
    "    log_phi1 = np.log(phi + 1e-15)\n",
    "    log_phi0 = np.log(1 - phi + 1e-15)\n",
    "    preds = []\n",
    "    for xi in X:\n",
    "        xi_col = xi.reshape(-1, 1)\n",
    "        # log likelihood for class 1\n",
    "        log_likelihood1 = -0.5 * np.sum(np.log(2 * np.pi * var_1) + ((xi_col - mu_1) ** 2) / var_1)\n",
    "        # log likelihood for class 0\n",
    "        log_likelihood0 = -0.5 * np.sum(np.log(2 * np.pi * var_0) + ((xi_col - mu_0) ** 2) / var_0)\n",
    "        log_p1 = log_phi1 + log_likelihood1\n",
    "        log_p0 = log_phi0 + log_likelihood0\n",
    "        preds.append(1 if log_p1 > log_p0 else 0)\n",
    "    return np.array(preds).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Fit Gaussian Naive Bayes parameters\n",
    "phi_nb, mu_0_nb, mu_1_nb, var_0_nb, var_1_nb = naive_bayes_fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "nb_preds = naive_bayes_predict(X_test, phi_nb, mu_0_nb, mu_1_nb, var_0_nb, var_1_nb)\n",
    "nb_accuracy = (nb_preds == y_test).mean()\n",
    "print(f\"Gaussian Naive Bayes test accuracy: {nb_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245194c",
   "metadata": {},
   "source": [
    "### Comparison & Visualization\n",
    "\n",
    "For a low‑dimensional visualization, we project the data onto the first two principal\n",
    "components using singular value decomposition.  We then plot the decision boundaries\n",
    "of GDA and Naive Bayes in this 2D space.  Note that the actual classification uses\n",
    "all features; the projection is for illustration only.\n",
    "\n",
    "Compute first two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a29369",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(X_train, full_matrices=False)\n",
    "X_train_pc = X_train @ Vt.T[:, :2]\n",
    "X_test_pc = X_test @ Vt.T[:, :2]\n",
    "\n",
    "# Fit models in PC space using only two features\n",
    "phi_pc, mu0_pc, mu1_pc, Sigma_pc = gda_fit(X_train_pc, y_train)\n",
    "phi_nb_pc, mu0_nb_pc, mu1_nb_pc, var0_nb_pc, var1_nb_pc = naive_bayes_fit(X_train_pc, y_train)\n",
    "\n",
    "# Create grid for decision boundary visualization\n",
    "x_min, x_max = X_train_pc[:, 0].min() - 1, X_train_pc[:, 0].max() + 1\n",
    "y_min, y_max = X_train_pc[:, 1].min() - 1, X_train_pc[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "gda_grid_pred = gda_predict(grid, phi_pc, mu0_pc, mu1_pc, Sigma_pc).reshape(xx.shape)\n",
    "nb_grid_pred = naive_bayes_predict(grid, phi_nb_pc, mu0_nb_pc, mu1_nb_pc, var0_nb_pc, var1_nb_pc).reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(xx, yy, gda_grid_pred, levels=[-0.5, 0.5, 1.5], alpha=0.3, cmap='bwr')\n",
    "plt.scatter(X_train_pc[y_train[:, 0] == 0][:, 0], X_train_pc[y_train[:, 0] == 0][:, 1], label=\"Class 0\", alpha=0.6)\n",
    "plt.scatter(X_train_pc[y_train[:, 0] == 1][:, 0], X_train_pc[y_train[:, 0] == 1][:, 1], label=\"Class 1\", alpha=0.6)\n",
    "plt.title(\"GDA Decision Boundary (2D PCA)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(xx, yy, nb_grid_pred, levels=[-0.5, 0.5, 1.5], alpha=0.3, cmap='bwr')\n",
    "plt.scatter(X_train_pc[y_train[:, 0] == 0][:, 0], X_train_pc[y_train[:, 0] == 0][:, 1], label=\"Class 0\", alpha=0.6)\n",
    "plt.scatter(X_train_pc[y_train[:, 0] == 1][:, 0], X_train_pc[y_train[:, 0] == 1][:, 1], label=\"Class 1\", alpha=0.6)\n",
    "plt.title(\"Naive Bayes Decision Boundary (2D PCA)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3207e6d",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Class Imbalance**: Modify the class prior \\(\\phi\\) artificially (e.g., set \\(\\phi = 0.9\\))\n",
    "   and observe how the decision boundary and accuracy change.\n",
    "2. **Multivariate vs. Naive**: Create a synthetic dataset where features are highly correlated.\n",
    "   Compare GDA and Naive Bayes performance.\n",
    "3. **Categorical Naive Bayes**: Implement a Bernoulli or multinomial naive Bayes classifier for text\n",
    "   classification.  Apply it to a toy document classification problem.\n",
    "4. **Regularized Covariance**: In GDA, add a small multiple of the identity matrix to \\(\\Sigma\\)\n",
    "   to improve numerical stability.  Investigate its impact on performance.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- **Generative vs. Discriminative**: generative models learn \\(P(x \\mid y) P(y)\\) and use Bayes' rule\n",
    "  to predict; discriminative models learn \\(P(y \\mid x)\\) directly.\n",
    "- **Gaussian Discriminant Analysis** assumes a multivariate normal distribution for each class\n",
    "  with a shared covariance matrix.  The resulting decision boundary is linear and\n",
    "  resembles logistic regression.\n",
    "- **Naive Bayes** assumes conditional independence of features given the class.  The\n",
    "  generative distribution factorizes, simplifying parameter estimation but potentially\n",
    "  reducing accuracy when features are correlated.\n",
    "- Both GDA and Naive Bayes require estimating class priors and means.  Naive Bayes also\n",
    "  estimates variances per feature.  Adding Laplace smoothing or small variances\n",
    "  prevents numerical issues.\n",
    "- Generative models can be extended to multimodal distributions (e.g., Gaussian mixtures)\n",
    "  or other exponential family distributions depending on the data type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
