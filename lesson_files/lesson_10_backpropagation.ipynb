{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f036eb",
   "metadata": {},
   "source": [
    "# Lesson 10 — Backpropagation in Depth\n",
    "\n",
    "Backpropagation is the workhorse algorithm that efficiently computes gradients in\n",
    "neural networks.  In this notebook we derive and implement backpropagation for a\n",
    "simple network with two hidden layers on the XOR problem, a classic example of a\n",
    "nonlinearly separable dataset.  We illustrate how the chain rule propagates\n",
    "gradients from the output back to earlier layers.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **XOR dataset**: generate a minimal binary classification problem.\n",
    "- **Network architecture**: two hidden layers and a sigmoid activation.\n",
    "- **Forward pass**: compute activations layer by layer.\n",
    "- **Backpropagation**: derive gradients for all parameters.\n",
    "- **Training loop**: stochastic gradient descent on the tiny dataset.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22a6ce",
   "metadata": {},
   "source": [
    "### Imports & Data Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62583b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=float)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=float)\n",
    "\n",
    "print(\"XOR dataset:\")\n",
    "print(np.hstack([X, y]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efad52",
   "metadata": {},
   "source": [
    "### Network Architecture & Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b725822",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "hidden_dim1 = 4\n",
    "hidden_dim2 = 4\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(input_dim, hidden_dim1) * 0.5\n",
    "b1 = np.zeros((1, hidden_dim1))\n",
    "W2 = np.random.randn(hidden_dim1, hidden_dim2) * 0.5\n",
    "b2 = np.zeros((1, hidden_dim2))\n",
    "W3 = np.random.randn(hidden_dim2, output_dim) * 0.5\n",
    "b3 = np.zeros((1, output_dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f20a0",
   "metadata": {},
   "source": [
    "### Activation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a: np.ndarray) -> np.ndarray:\n",
    "    return a * (1 - a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93ac11",
   "metadata": {},
   "source": [
    "### Forward Pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X: np.ndarray, W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, W3: np.ndarray, b3: np.ndarray):\n",
    "    \"\"\"Compute activations for all layers.\"\"\"\n",
    "    Z1 = X @ W1 + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    Z3 = A2 @ W3 + b3\n",
    "    A3 = sigmoid(Z3)  # final output in [0,1]\n",
    "    cache = (Z1, A1, Z2, A2, Z3, A3)\n",
    "    return A3, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be519a1c",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "We derive gradients using the chain rule.  Let the loss be the binary cross‑entropy:\n",
    "\\(L = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)})]\\).\n",
    "The derivative of the loss w.r.t. the output activation is \\(dA3 = -(y / A3 - (1 - y)/(1 - A3))\\).  We\n",
    "propagate this back through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd00b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X: np.ndarray, y_true: np.ndarray, cache, W1, W2, W3):\n",
    "    \"\"\"Compute gradients for all parameters via backpropagation.\"\"\"\n",
    "    Z1, A1, Z2, A2, Z3, A3 = cache\n",
    "    m = X.shape[0]\n",
    "    # Derivative of loss w.r.t. A3\n",
    "    dA3 = - (y_true / (A3 + 1e-15) - (1 - y_true) / (1 - A3 + 1e-15)) / m\n",
    "    # Layer 3\n",
    "    dZ3 = dA3 * sigmoid_derivative(A3)\n",
    "    dW3 = A2.T @ dZ3\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
    "    dA2 = dZ3 @ W3.T\n",
    "    # Layer 2\n",
    "    dZ2 = dA2 * sigmoid_derivative(A2)\n",
    "    dW2 = A1.T @ dZ2\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    dA1 = dZ2 @ W2.T\n",
    "    # Layer 1\n",
    "    dZ1 = dA1 * sigmoid_derivative(A1)\n",
    "    dW1 = X.T @ dZ1\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "    return dW1, db1, dW2, db2, dW3, db3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d103e0",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 10000\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    A3, cache = forward_pass(X, W1, b1, W2, b2, W3, b3)\n",
    "    # Loss (binary cross‑entropy)\n",
    "    loss = -np.mean(y * np.log(A3 + 1e-15) + (1 - y) * np.log(1 - A3 + 1e-15))\n",
    "    loss_history.append(loss)\n",
    "    # Backward\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backward_pass(X, y, cache, W1, W2, W3)\n",
    "    # Update\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    # Optionally print progress\n",
    "    if (epoch + 1) % 2000 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Binary Cross‑Entropy Loss\")\n",
    "plt.title(\"Training Loss for XOR Network\")\n",
    "plt.show()\n",
    "\n",
    "# Final predictions\n",
    "preds = (A3 >= 0.5).astype(int)\n",
    "accuracy = (preds == y).mean()\n",
    "print(f\"Final training accuracy on XOR: {accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06948095",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Activation Functions**: Replace the sigmoid activation with ReLU in the hidden layers\n",
    "   and observe the effect on convergence and required hidden units.\n",
    "2. **Network Depth**: Experiment with a single hidden layer vs. two hidden layers.\n",
    "   When does a deeper network perform better, and why is depth necessary for XOR?\n",
    "3. **Learning Rate Scheduling**: Implement a decaying learning rate schedule and\n",
    "   compare convergence rates.\n",
    "4. **Vectorization**: Extend this implementation to handle mini‑batches and larger\n",
    "   datasets.  Derive the gradients accordingly.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- Backpropagation applies the chain rule to efficiently compute gradients of a\n",
    "  network's parameters.  Each layer's gradient depends on the derivative of its\n",
    "  activation and the gradients of subsequent layers.\n",
    "- Multi‑layer networks can represent functions that are not linearly separable (e.g., XOR).\n",
    "  Depth enables compositional representations that capture complex patterns.\n",
    "- Binary cross‑entropy loss and sigmoid activations are appropriate for binary\n",
    "  classification; softmax and cross‑entropy generalize to multiclass.\n",
    "- Proper weight initialization and learning rate selection are critical for\n",
    "  convergence; too large a learning rate can cause divergence, while too small slows\n",
    "  learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
