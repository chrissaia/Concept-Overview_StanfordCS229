{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e340cf",
   "metadata": {},
   "source": [
    "# Lesson 15 — Reinforcement Learning: Value Iteration in Gridworld\n",
    "\n",
    "Reinforcement learning deals with sequential decision making in environments modeled\n",
    "as Markov decision processes (MDPs).  In this notebook we implement **value\n",
    "iteration**, a dynamic programming algorithm that computes the optimal policy by\n",
    "iteratively improving estimates of the state value function.  We demonstrate on a\n",
    "simple gridworld where the agent seeks to reach a goal while avoiding obstacles.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **MDP definition**: states, actions, transition probabilities and rewards.\n",
    "- **Value iteration algorithm**: Bellman optimality update.\n",
    "- **Policy extraction**: derive optimal actions from the value function.\n",
    "- **Visualization**: display the optimal value function and policy arrows.\n",
    "- **Exercises & interview summary**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d15f4",
   "metadata": {},
   "source": [
    "### Imports & Gridworld Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97abcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define gridworld dimensions\n",
    "grid_size = (5, 5)\n",
    "\n",
    "# Define rewards: -1 per step, 0 at goal, -10 at a pit state\n",
    "goal_state = (4, 4)\n",
    "pit_state = (2, 2)\n",
    "\n",
    "def reward(state):\n",
    "    if state == goal_state:\n",
    "        return 0\n",
    "    if state == pit_state:\n",
    "        return -10\n",
    "    return -1\n",
    "\n",
    "# Possible actions: up, down, left, right\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "# Transition model: deterministic for simplicity\n",
    "def next_state(state, action):\n",
    "    r, c = state\n",
    "    dr, dc = action\n",
    "    r_new = min(max(r + dr, 0), grid_size[0] - 1)\n",
    "    c_new = min(max(c + dc, 0), grid_size[1] - 1)\n",
    "    return (r_new, c_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b711f",
   "metadata": {},
   "source": [
    "### Value Iteration Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8260fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid_size, gamma=0.9, theta=1e-4):\n",
    "    \"\"\"Compute the optimal value function using value iteration.\"\"\"\n",
    "    V = np.zeros(grid_size)\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for r in range(grid_size[0]):\n",
    "            for c in range(grid_size[1]):\n",
    "                state = (r, c)\n",
    "                if state == goal_state:\n",
    "                    continue\n",
    "                v_old = V[state]\n",
    "                values = []\n",
    "                for action in actions:\n",
    "                    ns = next_state(state, action)\n",
    "                    v = reward(ns) + gamma * V[ns]\n",
    "                    values.append(v)\n",
    "                V[state] = max(values)\n",
    "                delta = max(delta, abs(v_old - V[state]))\n",
    "        iteration += 1\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def extract_policy(V, gamma=0.9):\n",
    "    \"\"\"Extract the optimal policy from the value function.\"\"\"\n",
    "    policy = np.zeros((*grid_size, len(actions)))\n",
    "    for r in range(grid_size[0]):\n",
    "        for c in range(grid_size[1]):\n",
    "            state = (r, c)\n",
    "            if state == goal_state:\n",
    "                continue\n",
    "            values = []\n",
    "            for action in actions:\n",
    "                ns = next_state(state, action)\n",
    "                v = reward(ns) + gamma * V[ns]\n",
    "                values.append(v)\n",
    "            best_action_idx = np.argmax(values)\n",
    "            policy[r, c, best_action_idx] = 1\n",
    "    return policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bce8cd2",
   "metadata": {},
   "source": [
    "### Running Value Iteration and Visualizing Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3dbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "V_opt = value_iteration(grid_size, gamma=gamma)\n",
    "policy_opt = extract_policy(V_opt, gamma=gamma)\n",
    "\n",
    "print(\"Optimal value function:\")\n",
    "print(V_opt)\n",
    "\n",
    "# Plot value function heatmap\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(V_opt, cmap='coolwarm')\n",
    "plt.colorbar(label=\"Value\")\n",
    "plt.title(\"Optimal State Value Function\")\n",
    "plt.scatter([pit_state[1]], [pit_state[0]], marker='x', color='black', s=100, label=\"Pit\")\n",
    "plt.scatter([goal_state[1]], [goal_state[0]], marker='*', color='gold', s=150, label=\"Goal\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot policy arrows\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(V_opt, cmap='coolwarm')\n",
    "for r in range(grid_size[0]):\n",
    "    for c in range(grid_size[1]):\n",
    "        if (r, c) == goal_state:\n",
    "            continue\n",
    "        a_idx = np.argmax(policy_opt[r, c])\n",
    "        dr, dc = actions[a_idx]\n",
    "        plt.arrow(c, r, 0.3 * dc, -0.3 * dr, head_width=0.2, head_length=0.2, fc='k', ec='k')\n",
    "plt.scatter([pit_state[1]], [pit_state[0]], marker='x', color='black', s=100, label=\"Pit\")\n",
    "plt.scatter([goal_state[1]], [goal_state[0]], marker='*', color='gold', s=150, label=\"Goal\")\n",
    "plt.title(\"Optimal Policy Arrows\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696ef38",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Discount Factor**: Vary \\(\\gamma\\) and observe its impact on the value function and policy.\n",
    "   A smaller \\(\\gamma\\) makes the agent more myopic.\n",
    "2. **Stochastic Transitions**: Modify `next_state` to include randomness (e.g. 80%\n",
    "   probability of intended move, 20% random move).  Update value iteration accordingly.\n",
    "3. **Policy Iteration**: Implement the policy iteration algorithm and compare its\n",
    "   convergence to value iteration.\n",
    "4. **Larger Gridworld**: Increase the grid size, add more obstacles and rewards,\n",
    "   and visualize the optimal policy.\n",
    "\n",
    "### Interview‑Ready Summary\n",
    "\n",
    "- Value iteration uses dynamic programming to compute the optimal state value function\n",
    "  by repeatedly applying the Bellman optimality operator until convergence.\n",
    "- The update rule considers the expected return of each action and chooses the\n",
    "  maximum, yielding the value of a state under the optimal policy.\n",
    "- Once the value function is computed, the optimal policy is extracted by choosing\n",
    "  the action that maximizes the expected return from each state.\n",
    "- The discount factor \\(\\gamma\\) balances immediate vs. future rewards.  Stochastic\n",
    "  transitions require summing over next states weighted by transition probabilities.\n",
    "- Policy iteration alternates between policy evaluation and policy improvement and\n",
    "  often converges faster than value iteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
