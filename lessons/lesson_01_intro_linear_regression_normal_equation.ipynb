{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 01: Linear Regression Basics + Normal Equation\n",
        "\n",
        "## Objectives\n",
        "- Revisit the supervised learning setup and CS229 notation.\n",
        "- Derive the least-squares objective for linear regression.\n",
        "- Solve linear regression with the normal equation.\n",
        "- Visualize residuals and the cost surface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From the notes: notation + objective\n",
        "We use the CS229 notation: training data \\(\\{(x^{(i)}, y^{(i)})\\}_{i=1}^m\\), feature dimension \\(n\\), parameters \\(\theta\\), and hypothesis \\(h_\theta(x) = \theta^T x\\) with \\(x_0 = 1\\).\n",
        "\n",
        "Least-squares objective:\n",
        "\\[\n",
        "J(\theta) = \frac{1}{2m} \\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2.\n",
        "\\]\n",
        "Normal equation solution:\n",
        "\\[\n",
        "\theta = (X^T X)^{-1} X^T y.\n",
        "\\]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intuition\n",
        "Linear regression finds the line (or hyperplane) that minimizes squared error. The normal equation gives the closed-form minimizer when \\(X^T X\\) is invertible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data\n",
        "We create a synthetic 1D dataset with noise so we can visualize the fitted line and residuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m np.random.seed(\u001b[32m42\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synthetic data\n",
        "m = 60\n",
        "X_raw = np.linspace(0, 10, m)\n",
        "y = 3.5 * X_raw + 2.0 + np.random.normal(0, 2.0, size=m)\n",
        "\n",
        "# Add bias term\n",
        "X = np.c_[np.ones(m), X_raw]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation: normal equation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normal equation\n",
        "XtX = X.T @ X\n",
        "theta = np.linalg.pinv(XtX) @ X.T @ y\n",
        "\n",
        "def predict(X, theta):\n",
        "    return X @ theta\n",
        "\n",
        "preds = predict(X, theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments\n",
        "We compare predictions and analyze residuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "residuals = y - preds\n",
        "mse = np.mean(residuals**2)\n",
        "print(f\"MSE: {mse:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(X_raw, y, label=\"data\", alpha=0.7)\n",
        "plt.plot(X_raw, preds, color=\"C1\", label=\"normal equation fit\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Linear regression fit\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(X_raw, residuals)\n",
        "plt.axhline(0, color=\"black\", linewidth=1)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"residual\")\n",
        "plt.title(\"Residuals\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways\n",
        "- Least squares gives a convex objective with a closed-form solution.\n",
        "- The normal equation is fast for small \\(n\\), but can be expensive for large feature sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explain it in an interview\n",
        "- Frame supervised learning as fitting \\(\theta\\) to minimize squared error.\n",
        "- Mention the normal equation and when it is preferable to iterative methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "1. What happens to the normal equation when \\(X^T X\\) is singular?\n",
        "2. Add a quadratic feature and compare the fit.\n",
        "3. Implement feature scaling and check the effect on the conditioning of \\(X^T X\\)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
